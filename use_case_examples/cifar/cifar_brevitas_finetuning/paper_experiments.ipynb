{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing Paper _Programmable Bootstrapping Enables Efficient Homomorphic Inference of Deep Neural Networks_ : Benchmarking Results\n",
    "\n",
    "This notebook replicates experiments from the paper [_Programmable Bootstrapping Enables Efficient Homomorphic Inference of Deep Neural Networks_](https://pypi.org/project/concrete-ml/\n",
    "), published in 2019. \n",
    "It provides an in-depth analysis of the deep neural network architectures NN-20 and NN-50, along with their training processes using floating point precisio and their [quantization](https://docs.zama.ai/concrete-ml/explanations/quantization) using the Quantization Aware Training (QAT) and Post Training Quantization (PTQ) methods. \n",
    "\n",
    "We compare the original paper's findings with the results from the latest version of [Concrete ML](https://whitepaper.zama.ai/). This comparison highlight the significant advancements made by Concrete ML, particularly in execution speed while preserving model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Optional, Tuple\n",
    "\n",
    "import brevitas\n",
    "import brevitas.nn as qnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from brevitas import config\n",
    "from concrete.fhe.compilation import Configuration\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "from concrete import fhe\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model, compile_torch_model\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "All networks begin with a convolutional layer configured with `in_channel=1, out_channels=1, kernel_size=3, stride=1, padding_mode='replicate'`. At the time of writing this notebook, `padding=1` is not yet supported; as a workaround, padding is added during the data loading transformation process. \n",
    "\n",
    "This is followed by 20 linear layers of 92 neurones with ReLU activation for NN-20, and 50 layers for NN-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "# Input size\n",
    "INPUT_IMG_SIZE = 28\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# MNIST data normalization\n",
    "MEAN = STD = 0.5\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_MAPS = [\n",
    "    # The Bravitas.QuantIdentiy layer, only used in the quant NN, aims to quantize the input.\n",
    "    # (\"I\",),\n",
    "    # Convolution layer, with:\n",
    "    #  in_channel=1, out_channels=1, kernel_size=3, stride=1, padding_mode='replicate'\n",
    "    (\"C\", 1, 1, 3, 1, \"replicate\"),\n",
    "]\n",
    "\n",
    "# The article presents 3 neural network depths. In this notebook, we focus NN-20 and NN-50\n",
    "# architectures. The parameter `nb_layers`: controls the depth of the NN.\n",
    "LINEAR_LAYERS = lambda nb_layers, output_size: (\n",
    "    [\n",
    "        (\"R\",),\n",
    "        (\"L\", INPUT_IMG_SIZE * INPUT_IMG_SIZE, 92),\n",
    "        (\"B\", 92),\n",
    "    ]\n",
    "    + [\n",
    "        (\"R\",),\n",
    "        (\"L\", 92, 92),\n",
    "        (\"B\", 92),\n",
    "    ]\n",
    "    * (nb_layers - 2)\n",
    "    + [(\"L\", 92, output_size)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 MNIST Neural Nerwork "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fp32MNIST(torch.nn.Module):\n",
    "    \"\"\"MNIST Torch model.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_layers: int, output_size: int = 10):\n",
    "        super(Fp32MNIST, self).__init__()\n",
    "        \"\"\"MNIST Torch model.\n",
    "\n",
    "        Args:\n",
    "            nb_layers (int): Number of layers.\n",
    "            output_size (int): Number of classes.\n",
    "        \"\"\"\n",
    "        self.nb_layers = nb_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        def make_layers(t):\n",
    "            if t[0] == \"C\":\n",
    "                return torch.nn.Conv2d(\n",
    "                    in_channels=t[1],\n",
    "                    out_channels=t[2],\n",
    "                    kernel_size=t[3],\n",
    "                    stride=t[4],\n",
    "                    padding_mode=t[5],\n",
    "                )\n",
    "            if t[0] == \"L\":\n",
    "                return torch.nn.Linear(in_features=t[1], out_features=t[2])\n",
    "            if t[0] == \"R\":\n",
    "                return torch.nn.ReLU()\n",
    "            if t[0]:\n",
    "                return torch.nn.BatchNorm1d(t[1])\n",
    "\n",
    "            raise NameError(f\"'{t}' not defined\")\n",
    "\n",
    "        # QuantIdentity layers are ignored in the floationg point architecture.\n",
    "        self.features_maps = torch.nn.Sequential(\n",
    "            *[make_layers(t) for t in FEATURES_MAPS if t[0] != \"I\"]\n",
    "        )\n",
    "        self.linears = torch.nn.Sequential(\n",
    "            *[\n",
    "                make_layers(t)\n",
    "                for t in LINEAR_LAYERS(self.nb_layers, self.output_size)\n",
    "                if t[0] != \"I\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_maps(x)\n",
    "        x = torch.nn.Flatten()(x)\n",
    "        x = self.linears(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized MNIST Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantMNIST(torch.nn.Module):\n",
    "    \"\"\"Quantized MNIST network with Brevitas.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_bits: int,\n",
    "        nb_layers: int,\n",
    "        output_size: int = 10,\n",
    "        act_quant: brevitas.quant = brevitas.quant.Int8ActPerTensorFloat,\n",
    "        weight_quant: brevitas.quant = brevitas.quant.Int8WeightPerTensorFloat,\n",
    "    ):\n",
    "        \"\"\"A quantized network with Brevitas.\n",
    "\n",
    "        Args:\n",
    "            n_bits (int): Precision bit of quantization.\n",
    "            nb_layers (int): Number of layers.\n",
    "            output_size (int): Number of classes\n",
    "            act_quant (brevitas.quant): Quantization protocol of activations.\n",
    "            weight_quant (brevitas.quant): Quantization protocol of the weights.\n",
    "        \"\"\"\n",
    "        super(QuantMNIST, self).__init__()\n",
    "\n",
    "        self.n_bits = n_bits\n",
    "        self.nb_layers = nb_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        def tuple2quantlayer(t):\n",
    "            if t[0] == \"R\":\n",
    "                return qnn.QuantReLU(\n",
    "                    return_quant_tensor=True, bit_width=n_bits, act_quant=act_quant\n",
    "                )\n",
    "            if t[0] == \"C\":\n",
    "                return qnn.QuantConv2d(\n",
    "                    t[1],\n",
    "                    t[2],\n",
    "                    kernel_size=t[3],\n",
    "                    weight_bit_width=2,\n",
    "                    weight_quant=weight_quant,\n",
    "                    return_quant_tensor=True,\n",
    "                )\n",
    "            if t[0] == \"L\":\n",
    "                return qnn.QuantLinear(\n",
    "                    in_features=t[1],\n",
    "                    out_features=t[2],\n",
    "                    weight_bit_width=n_bits,\n",
    "                    weight_quant=weight_quant,\n",
    "                    bias=True,\n",
    "                    return_quant_tensor=True,\n",
    "                )\n",
    "            if t[0] == \"I\":\n",
    "                identity_quant = t[1] if len(t) == 2 else n_bits\n",
    "                return qnn.QuantIdentity(\n",
    "                    bit_width=identity_quant, act_quant=act_quant, return_quant_tensor=True\n",
    "                )\n",
    "            if t[0] == \"B\":\n",
    "                return torch.nn.BatchNorm1d(t[1])\n",
    "\n",
    "        self.features_maps = torch.nn.Sequential(\n",
    "            *[tuple2quantlayer(t) for t in FEATURES_MAPS if t[0]]\n",
    "        )\n",
    "\n",
    "        # self.identity1 and self.identity2 are used to encapsulate the `torch.flatten`.\n",
    "        self.identity1 = qnn.QuantIdentity(\n",
    "            bit_width=n_bits, act_quant=act_quant, return_quant_tensor=True\n",
    "        )\n",
    "        self.identity2 = qnn.QuantIdentity(\n",
    "            bit_width=n_bits, act_quant=act_quant, return_quant_tensor=True\n",
    "        )\n",
    "\n",
    "        # QuantIdentity layers are taken into account\n",
    "        self.linears = torch.nn.Sequential(\n",
    "            *[tuple2quantlayer(t) for t in LINEAR_LAYERS(self.nb_layers, self.output_size)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_maps(x)\n",
    "        x = self.identity1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.identity2(x)\n",
    "        x = self.linears(x)\n",
    "        return x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(data_loader: DataLoader, n: int = 10) -> None:\n",
    "    \"\"\"Visualize some images from a given data loader.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): Data loader.\n",
    "        n (int): Limit the number of images to display to `n`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Class names\n",
    "    class_names = data_loader.dataset.classes\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    images, labels = next(iter(data_loader))\n",
    "\n",
    "    # Make a grid from batch and rotate to get a valid shape for imshow\n",
    "    images = make_grid(images[:n], nrow=n).permute((1, 2, 0))\n",
    "    # Remove the previous normalization\n",
    "    images = images * np.array(STD) + np.array(MEAN)\n",
    "\n",
    "    ax.imshow(images)\n",
    "\n",
    "    ax.set_title(\"\".join([f\"{class_names[img]:<13}\" for img in labels[:n]]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_keys(pre_trained_weights: Dict, model: torch.nn.Module, device: str) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Initialize the quantized model with pre-trained fp32 weights.\n",
    "\n",
    "    Args:\n",
    "        pre_trained_weights (Dict): The state_dict of the pre-trained fp32 model.\n",
    "        model (nn.Module): The Brevitas model.\n",
    "        device (str): Device type.\n",
    "\n",
    "    Returns:\n",
    "        Callable: The quantized model with the pre-trained state_dict.\n",
    "    \"\"\"\n",
    "\n",
    "    # Brevitas requirement to ignore missing keys\n",
    "    config.IGNORE_MISSING_KEYS = True\n",
    "\n",
    "    old_keys = list(pre_trained_weights.keys())\n",
    "    new_keys = list(model.state_dict().keys())\n",
    "    new_state_dict = OrderedDict()\n",
    "\n",
    "    for old_key, new_key in zip(old_keys, new_keys):\n",
    "        new_state_dict[new_key] = pre_trained_weights[old_key]\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    param: Dict,\n",
    "    step: int = 1,\n",
    "    device: str = \"cpu\",\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Training the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): A PyTorch or Brevitas network.\n",
    "        train_loader (DataLoader): The training set.\n",
    "        test_loader (DataLoader): The test set.\n",
    "        param (Dict): Set of hyper-parameters to use depending on whether\n",
    "            CIFAR-10 or CIFAR-100 is used.\n",
    "        step (int): Display the loss and accuracy every `epoch % step`.\n",
    "        device (str): Device type.\n",
    "    Returns:\n",
    "        nn.Module: the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    if param[\"seed\"]:\n",
    "\n",
    "        torch.manual_seed(param[\"seed\"])\n",
    "        random.seed(param[\"seed\"])\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=param[\"lr\"])\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=param[\"milestones\"], gamma=param[\"gamma\"]\n",
    "    )\n",
    "\n",
    "    # Save the state_dict\n",
    "    dir = Path(\".\") / param[\"dir\"] / param[\"training\"]\n",
    "    dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # To avoid breaking up the tqdm bar\n",
    "    with tqdm(total=param[\"epochs\"], file=sys.stdout) as pbar:\n",
    "\n",
    "        for i in range(param[\"epochs\"]):\n",
    "            # Train the model\n",
    "            model.train()\n",
    "            loss_batch_train, accuracy_batch_train = [], []\n",
    "\n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                yhat = model(x)\n",
    "                loss_train = param[\"criterion\"](yhat, y)\n",
    "                loss_train.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_batch_train.append(loss_train.item())\n",
    "                accuracy_batch_train.extend((yhat.argmax(1) == y).cpu().float().tolist())\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            param[\"accuracy_train\"].append(np.mean(accuracy_batch_train))\n",
    "            param[\"loss_train_history\"].append(np.mean(loss_batch_train))\n",
    "\n",
    "            # Evaluation during training:\n",
    "            # Disable autograd engine (no backpropagation)\n",
    "            # To reduce memory usage and to speed up computations\n",
    "            with torch.no_grad():\n",
    "                # Notify batchnormalization & dropout layers to work in eval mode\n",
    "                model.eval()\n",
    "                loss_batch_test, accuracy_batch_test = [], []\n",
    "                for x, y in test_loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    loss_test = param[\"criterion\"](yhat, y)\n",
    "                    loss_batch_test.append(loss_test.item())\n",
    "                    accuracy_batch_test.extend((yhat.argmax(1) == y).cpu().float().tolist())\n",
    "\n",
    "                param[\"accuracy_test\"].append(np.mean(accuracy_batch_test))\n",
    "                param[\"loss_test_history\"].append(np.mean(loss_batch_test))\n",
    "\n",
    "            if i % step == 0:\n",
    "                pbar.write(\n",
    "                    f\"Epoch {i:2}: Train loss = {param['loss_train_history'][-1]:.4f} \"\n",
    "                    f\"VS Test loss = {param['loss_test_history'][-1]:.4f} - \"\n",
    "                    f\"Accuracy train: {param['accuracy_train'][-1]:.4f} \"\n",
    "                    f\"VS Accuracy test: {param['accuracy_test'][-1]:.4f}\"\n",
    "                )\n",
    "                pbar.update(step)\n",
    "\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                f\"{dir}/{param['dataset_name']}_{param['training']}_state_dict.pt\",\n",
    "            )\n",
    "\n",
    "    print(\"Save in:\", f\"{dir}/{param['dataset_name']}_{param['training']}_state_dict.pt\")\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(), f\"{dir}/{param['dataset_name']}_{param['training']}_state_dict.pt\"\n",
    "    )\n",
    "    import pickle as pkl\n",
    "\n",
    "    with open(f\"{dir}/{param['dataset_name']}_history.pkl\", \"wb\") as f:\n",
    "        pkl.dump(param, f)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_inference(\n",
    "    model: torch.nn.Module,\n",
    "    data: DataLoader,\n",
    "    device: str = \"cpu\",\n",
    "    verbose: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"Returns the `top_k` accuracy.\n",
    "\n",
    "    Args:\n",
    "        model (ntorch.n.Module): A PyTorch or Brevitas network.\n",
    "        data (DataLoader): The test or evaluation set.\n",
    "        device (str): Device type.\n",
    "        verbose (bool): For display.\n",
    "    Returns:\n",
    "        float: The top_k accuracy.\n",
    "    \"\"\"\n",
    "    correct = []\n",
    "    total_example = 0\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x, y in tqdm(data, disable=verbose is False):\n",
    "            x, y = x.to(device), y\n",
    "            yhat = model(x).cpu().detach()\n",
    "            correct.append(yhat.argmax(1) == y)\n",
    "            total_example += len(x)\n",
    "\n",
    "    return np.mean(np.vstack(correct), dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(\n",
    "    model: Callable,\n",
    "    compile_type: str,\n",
    "    data_calibration,\n",
    "    data_loader,\n",
    "    ptq_bits: int = None,\n",
    "):\n",
    "\n",
    "    if compile_type == \"qat\":\n",
    "        assert hasattr(model, \"n_bits\"), \"The model must be a Brevitas neural network.\"\n",
    "        bits = model.n_bits\n",
    "        compile_n_bits = None\n",
    "        nb_layers = model.nb_layers\n",
    "        compile_function = compile_brevitas_qat_model\n",
    "    else:\n",
    "        assert not hasattr(model, \"n_bits\"), \"The model must be a Torch neural network.\"\n",
    "        bits = compile_n_bits = ptq_bits\n",
    "        nb_layers = model.nb_layers\n",
    "        compile_function = compile_torch_model\n",
    "\n",
    "    history = []\n",
    "    filename = f\"history_{nb_layers=}_v2.csv\"\n",
    "    headers = [\n",
    "        \"Compile_type\",\n",
    "        \"Number_of_layers\",\n",
    "        \"Bits\",\n",
    "        \"threshold\",\n",
    "        \"max_bits\",\n",
    "        \"Mean_FP32_accuracy\",\n",
    "        \"Mean_disable_accuracy\",\n",
    "        \"Mean_simulate_accuracy\",\n",
    "        \"FHE_timing\",\n",
    "    ]\n",
    "\n",
    "    thresholds = [\n",
    "        {\"n_bits\": 5, \"method\": fhe.Exactness.APPROXIMATE},\n",
    "        {\"n_bits\": 5, \"method\": \"EXACT\"},\n",
    "        {\"n_bits\": 6, \"method\": \"APPROXIMATE\"},\n",
    "        {\"n_bits\": 6, \"method\": \"EXACT\"},\n",
    "        {\"n_bits\": 7, \"method\": \"APPROXIMATE\"},\n",
    "        {\"n_bits\": 7, \"method\": \"EXACT\"},\n",
    "    ]\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(headers)\n",
    "\n",
    "    with open(filename, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for j, threshold in tqdm(enumerate(thresholds)):\n",
    "\n",
    "            fhe_timing = -1\n",
    "            history_fp32_predictions = []\n",
    "            history_disable_predictions = []\n",
    "            history_simulat_predictions = []\n",
    "\n",
    "            print(threshold)\n",
    "            if 1:\n",
    "                q_module = compile_function(\n",
    "                    model.to(\"cpu\"),\n",
    "                    torch_inputset=data_calibration,\n",
    "                    n_bits=compile_n_bits,\n",
    "                    rounding_threshold_bits=threshold,\n",
    "                )\n",
    "\n",
    "                max_bits = q_module.fhe_circuit.graph.maximum_integer_bit_width()\n",
    "                print(f\"{j=}: {max_bits=} after compilation\")\n",
    "\n",
    "                for i, (data, labels) in tqdm(enumerate(data_loader)):\n",
    "\n",
    "                    fp32_predictions = model(data).cpu().detach()\n",
    "                    history_fp32_predictions.extend(fp32_predictions.argmax(1) == labels)\n",
    "\n",
    "                    data, labels = data.detach().cpu().numpy(), labels.detach().cpu().numpy()\n",
    "\n",
    "                    disable_predictions = q_module.forward(data, fhe=\"disable\")\n",
    "                    history_disable_predictions.extend(disable_predictions.argmax(1) == labels)\n",
    "\n",
    "                    simulat_predictions = q_module.forward(data, fhe=\"simulate\")\n",
    "                    history_simulat_predictions.extend(simulat_predictions.argmax(1) == labels)\n",
    "\n",
    "                    if i == 0:\n",
    "                        start_time = time.time()\n",
    "                        q_module.forward(data[0, None], fhe=\"execute\")\n",
    "                        fhe_timing = (time.time() - start_time) / 60.0\n",
    "\n",
    "                row = [\n",
    "                    compile_type,\n",
    "                    nb_layers,\n",
    "                    bits,\n",
    "                    threshold,\n",
    "                    max_bits,\n",
    "                    np.mean(history_fp32_predictions),\n",
    "                    np.mean(history_disable_predictions),\n",
    "                    np.mean(history_simulat_predictions),\n",
    "                    fhe_timing,\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                row = [compile_type, nb_layers, bits, threshold, -1, -1, -1, -1, -1]\n",
    "\n",
    "            writer.writerow(row)\n",
    "            print(f\"{j=}: {row=}\")\n",
    "            history.append(dict(zip(headers, row)))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 30, 30])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Pad(1, padding_mode=\"edge\"),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((MEAN,), (STD,)),\n",
    "        transforms.GaussianBlur(kernel_size=(3, 3)),\n",
    "    ]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Pad(1, padding_mode=\"edge\"),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((MEAN,), (STD,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(download=True, root=\"./data\", train=True, transform=train_transform)\n",
    "\n",
    "test_dataset = datasets.MNIST(download=True, root=\"./data\", train=False, transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    generator=g,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    generator=g,\n",
    ")\n",
    "\n",
    "# The calibration data-set is used during the compilation step\n",
    "# Where, the compiler requires an exhaustive set of data to evaluate the maximum integer bit-width\n",
    "# within the graph.\n",
    "data_calibration = next(iter(train_loader))[0]\n",
    "\n",
    "plot_dataset(train_loader)\n",
    "\n",
    "data_calibration.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP32 MNIST 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 - with 20-NN: acc_train=99.331% vs acc_test=98.137%\n"
     ]
    }
   ],
   "source": [
    "nb_layers, train_model = 20, False\n",
    "\n",
    "param = {\n",
    "    \"training\": \"fp32\",\n",
    "    \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "    \"accuracy_test\": [],\n",
    "    \"accuracy_train\": [],\n",
    "    \"loss_test_history\": [],\n",
    "    \"loss_train_history\": [],\n",
    "    \"dir\": f\"./checkpoints/MNIST/NLP_{nb_layers}/\",\n",
    "}\n",
    "\n",
    "fp32_mnist = Fp32MNIST(nb_layers=nb_layers).to(device)\n",
    "checkpoint = torch.load(\n",
    "    f\"checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt\", map_location=device\n",
    ")\n",
    "\n",
    "if train_model:\n",
    "    param[\"training\"] = \"fp32\"\n",
    "    param[\"epochs\"] = 30\n",
    "    param[\"lr\"] = 0.01\n",
    "    param[\"milestones\"] = [15, 24]\n",
    "    param[\"gamma\"] = 0.1\n",
    "\n",
    "    fp32_mnist = train(fp32_mnist, train_loader, test_loader, param, device=device)\n",
    "\n",
    "else:\n",
    "    checkpoint = torch.load(\n",
    "        f\"checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt\", map_location=device\n",
    "    )\n",
    "    fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "acc_train = torch_inference(fp32_mnist, train_loader, device=device)\n",
    "acc_test = torch_inference(fp32_mnist, test_loader, device=device)\n",
    "\n",
    "print(f\"FP32 - with {fp32_mnist.nb_layers}-NN: {acc_train=:.3%} vs {acc_test=:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quant MNIST 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before fine-tuning: acc_before_ft=9.746%\n",
      "Maximum bit-width in the circuit: 13\n",
      "\n",
      "With 4-bits and 20-nb_layers: acc_after_ft_train=97.287% vs acc_after_ft_test=97.175%\n",
      "Maximum bit-width in the circuit: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bits, finetune_model, nb_layers = 4, False, 20\n",
    "\n",
    "quant_mnist = QuantMNIST(n_bits=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "quant_mnist = mapping_keys(checkpoint, quant_mnist, device=\"cpu\")\n",
    "acc_before_ft = torch_inference(quant_mnist, test_loader, device=device)\n",
    "\n",
    "print(f\"Accuracy before fine-tuning: {acc_before_ft=:.3%}\")\n",
    "\n",
    "qmodel = compile_brevitas_qat_model(\n",
    "    quant_mnist.to(\"cpu\"),\n",
    "    # Training\n",
    "    torch_inputset=data_calibration,\n",
    "    show_mlir=False,\n",
    "    rounding_threshold_bits=6,\n",
    ")\n",
    "\n",
    "print(f\"Maximum bit-width in the circuit: {qmodel.fhe_circuit.graph.maximum_integer_bit_width()}\\n\")\n",
    "\n",
    "if finetune_model:\n",
    "    assert str(quant_mnist.nb_layers) in param[\"dir\"]\n",
    "    param[\"training\"] = f\"quant_mnist_{bits=}\"\n",
    "    param[\"epochs\"] = 5\n",
    "    param[\"lr\"] = 0.1\n",
    "    param[\"milestones\"] = [1, 3]\n",
    "    param[\"gamma\"] = 0.1\n",
    "\n",
    "    quant_mnist_20 = train(quant_mnist, train_loader, test_loader, param, device=device)\n",
    "\n",
    "path = (\n",
    "    f\"checkpoints/MNIST/NLP_{nb_layers}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    ")\n",
    "\n",
    "quant_mnist = QuantMNIST(n_bits=bits, nb_layers=nb_layers).to(device)\n",
    "quant_mnist.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "acc_after_ft_train = torch_inference(quant_mnist, train_loader, device=device)\n",
    "acc_after_ft_test = torch_inference(quant_mnist, test_loader, device=device)\n",
    "\n",
    "print(\n",
    "    f\"With {quant_mnist.n_bits}-bits and {quant_mnist.nb_layers}-nb_layers: \"\n",
    "    f\"{acc_after_ft_train=:.3%} vs {acc_after_ft_test=:.3%}\"\n",
    ")\n",
    "qmodel = compile_brevitas_qat_model(\n",
    "    quant_mnist.to(\"cpu\"),\n",
    "    # Training\n",
    "    torch_inputset=data_calibration,\n",
    "    show_mlir=False,\n",
    "    rounding_threshold_bits=6,\n",
    ")\n",
    "\n",
    "print(f\"Maximum bit-width in the circuit: {qmodel.fhe_circuit.graph.maximum_integer_bit_width()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 5-bits and 20-nb_layers: acc_after_ft_train=98.007% vs acc_after_ft_test=97.726%\n",
      "Maximum bit-width in the circuit: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bits, nb_layers = 5, 20\n",
    "\n",
    "path = (\n",
    "    f\"checkpoints/MNIST/NLP_{nb_layers}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    ")\n",
    "\n",
    "quant_mnist = QuantMNIST(n_bits=bits, nb_layers=nb_layers).to(device)\n",
    "quant_mnist.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "qmodel = compile_brevitas_qat_model(\n",
    "    quant_mnist.to(\"cpu\"),\n",
    "    torch_inputset=data_calibration,\n",
    "    show_mlir=False,\n",
    "    rounding_threshold_bits=6,\n",
    ")\n",
    "\n",
    "acc_after_ft_train = torch_inference(quant_mnist, train_loader, device=device)\n",
    "acc_after_ft_test = torch_inference(quant_mnist, test_loader, device=device)\n",
    "\n",
    "print(\n",
    "    f\"With {quant_mnist.n_bits}-bits and {quant_mnist.nb_layers}-nb_layers: \"\n",
    "    f\"{acc_after_ft_train=:.3%} vs {acc_after_ft_test=:.3%}\"\n",
    ")\n",
    "\n",
    "print(f\"Maximum bit-width in the circuit: {qmodel.fhe_circuit.graph.maximum_integer_bit_width()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmark for NN-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['checkpoints/MNIST/NLP_20/quant_mnist_bits=4/MNIST_quant_mnist_bits=4_state_dict.pt', 'checkpoints/MNIST/NLP_20/quant_mnist_bits=5/MNIST_quant_mnist_bits=5_state_dict.pt']\n",
      "checkpoints/MNIST/NLP_20/quant_mnist_bits=4/MNIST_quant_mnist_bits=4_state_dict.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_bits': 5, 'method': <Exactness.APPROXIMATE: 1>}\n",
      "j=0: max_bits=15 after compilation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nb_layers = 20\n",
    "\n",
    "fp32_mnist = Fp32MNIST(nb_layers=nb_layers).to(device)\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    f\"checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt\", map_location=device\n",
    ")\n",
    "fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "state_dicts = glob(f\"checkpoints/MNIST/NLP_{nb_layers}/quant_mnist_bits=*/*_state_dict.pt\")\n",
    "\n",
    "print(state_dicts)\n",
    "for state_dict in state_dicts:\n",
    "\n",
    "    if \"3\" in state_dict:\n",
    "        continue\n",
    "    print(state_dict)\n",
    "\n",
    "    quant_mnist = QuantMNIST(n_bits=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "    quant_mnist.load_state_dict(torch.load(state_dict, map_location=device))\n",
    "    history = run_benchmark(quant_mnist, \"qat\", data_calibration, test_loader, ptq_bits=None)\n",
    "\n",
    "# for ptq_bits in [3, 4, 5, 6]:\n",
    "\n",
    "#     history = run_benchmark(fp32_mnist, \"ptq\", data_calibration, test_loader, ptq_bits=ptq_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compile_type</th>\n",
       "      <th>Number_of_layers</th>\n",
       "      <th>Bits</th>\n",
       "      <th>threshold</th>\n",
       "      <th>max_bits</th>\n",
       "      <th>Mean_FP32_accuracy</th>\n",
       "      <th>Mean_disable_accuracy</th>\n",
       "      <th>Mean_simulate_accuracy</th>\n",
       "      <th>FHE_timing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977564</td>\n",
       "      <td>0.978265</td>\n",
       "      <td>0.976663</td>\n",
       "      <td>5.312609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977063</td>\n",
       "      <td>0.972456</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>2.695160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977163</td>\n",
       "      <td>0.907352</td>\n",
       "      <td>0.907352</td>\n",
       "      <td>2.412440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977564</td>\n",
       "      <td>0.587039</td>\n",
       "      <td>0.568810</td>\n",
       "      <td>2.039992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Exactness.APPROXIMATE</td>\n",
       "      <td>15</td>\n",
       "      <td>0.976062</td>\n",
       "      <td>0.097456</td>\n",
       "      <td>0.097456</td>\n",
       "      <td>1.925832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Compile_type  Number_of_layers  Bits              threshold  max_bits  \\\n",
       "0          ptq                20   5.0                      7        15   \n",
       "1          ptq                20   5.0                      6        15   \n",
       "2          ptq                20   5.0                      5        15   \n",
       "3          ptq                20   5.0                      4        15   \n",
       "4          ptq                20   5.0  Exactness.APPROXIMATE        15   \n",
       "\n",
       "   Mean_FP32_accuracy  Mean_disable_accuracy  Mean_simulate_accuracy  \\\n",
       "0            0.977564               0.978265                0.976663   \n",
       "1            0.977063               0.972456                0.971955   \n",
       "2            0.977163               0.907352                0.907352   \n",
       "3            0.977564               0.587039                0.568810   \n",
       "4            0.976062               0.097456                0.097456   \n",
       "\n",
       "   FHE_timing  \n",
       "0    5.312609  \n",
       "1    2.695160  \n",
       "2    2.412440  \n",
       "3    2.039992  \n",
       "4    1.925832  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compile_type</th>\n",
       "      <th>Number_of_layers</th>\n",
       "      <th>Bits</th>\n",
       "      <th>threshold</th>\n",
       "      <th>max_bits</th>\n",
       "      <th>Mean_FP32_accuracy</th>\n",
       "      <th>Mean_disable_accuracy</th>\n",
       "      <th>Mean_simulate_accuracy</th>\n",
       "      <th>FHE_timing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>qat</td>\n",
       "      <td>20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.957833</td>\n",
       "      <td>0.735477</td>\n",
       "      <td>0.735477</td>\n",
       "      <td>1.269313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qat</td>\n",
       "      <td>20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.961639</td>\n",
       "      <td>0.831230</td>\n",
       "      <td>0.831230</td>\n",
       "      <td>1.385299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977163</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.695613</td>\n",
       "      <td>2.076515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>qat</td>\n",
       "      <td>20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0.958534</td>\n",
       "      <td>0.969551</td>\n",
       "      <td>0.966046</td>\n",
       "      <td>2.220344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>qat</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.972155</td>\n",
       "      <td>0.895433</td>\n",
       "      <td>0.883313</td>\n",
       "      <td>2.310165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977163</td>\n",
       "      <td>0.948417</td>\n",
       "      <td>0.948417</td>\n",
       "      <td>2.368403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977163</td>\n",
       "      <td>0.907352</td>\n",
       "      <td>0.907352</td>\n",
       "      <td>2.393186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977163</td>\n",
       "      <td>0.907352</td>\n",
       "      <td>0.907352</td>\n",
       "      <td>2.412440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977364</td>\n",
       "      <td>0.972957</td>\n",
       "      <td>0.972957</td>\n",
       "      <td>2.639031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977063</td>\n",
       "      <td>0.972456</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>2.675938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977063</td>\n",
       "      <td>0.972456</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>2.695160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>qat</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0.970553</td>\n",
       "      <td>0.966146</td>\n",
       "      <td>0.966046</td>\n",
       "      <td>2.853394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977564</td>\n",
       "      <td>0.978265</td>\n",
       "      <td>0.976663</td>\n",
       "      <td>4.931353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.978866</td>\n",
       "      <td>0.978365</td>\n",
       "      <td>0.976763</td>\n",
       "      <td>4.994058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ptq</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.977564</td>\n",
       "      <td>0.978265</td>\n",
       "      <td>0.976663</td>\n",
       "      <td>5.312609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>qat</td>\n",
       "      <td>20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.962039</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>5.583628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>qat</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>0.970553</td>\n",
       "      <td>0.975260</td>\n",
       "      <td>0.975160</td>\n",
       "      <td>7.011712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Compile_type  Number_of_layers  Bits threshold  max_bits  \\\n",
       "20          qat                20   4.0         4        14   \n",
       "19          qat                20   4.0         5        14   \n",
       "14          ptq                20   5.0         4        15   \n",
       "18          qat                20   4.0         6        14   \n",
       "25          qat                20   5.0         5        16   \n",
       "13          ptq                20   5.0         5        15   \n",
       "7           ptq                20   5.0         5        15   \n",
       "2           ptq                20   5.0         5        15   \n",
       "12          ptq                20   5.0         6        15   \n",
       "6           ptq                20   5.0         6        15   \n",
       "1           ptq                20   5.0         6        15   \n",
       "24          qat                20   5.0         6        16   \n",
       "5           ptq                20   5.0         7        15   \n",
       "11          ptq                20   5.0         7        15   \n",
       "0           ptq                20   5.0         7        15   \n",
       "17          qat                20   4.0         7        14   \n",
       "23          qat                20   5.0         7        16   \n",
       "\n",
       "    Mean_FP32_accuracy  Mean_disable_accuracy  Mean_simulate_accuracy  \\\n",
       "20            0.957833               0.735477                0.735477   \n",
       "19            0.961639               0.831230                0.831230   \n",
       "14            0.977163               0.703125                0.695613   \n",
       "18            0.958534               0.969551                0.966046   \n",
       "25            0.972155               0.895433                0.883313   \n",
       "13            0.977163               0.948417                0.948417   \n",
       "7             0.977163               0.907352                0.907352   \n",
       "2             0.977163               0.907352                0.907352   \n",
       "12            0.977364               0.972957                0.972957   \n",
       "6             0.977063               0.972456                0.971955   \n",
       "1             0.977063               0.972456                0.971955   \n",
       "24            0.970553               0.966146                0.966046   \n",
       "5             0.977564               0.978265                0.976663   \n",
       "11            0.978866               0.978365                0.976763   \n",
       "0             0.977564               0.978265                0.976663   \n",
       "17            0.962039               0.971955                0.971955   \n",
       "23            0.970553               0.975260                0.975160   \n",
       "\n",
       "    FHE_timing  \n",
       "20    1.269313  \n",
       "19    1.385299  \n",
       "14    2.076515  \n",
       "18    2.220344  \n",
       "25    2.310165  \n",
       "13    2.368403  \n",
       "7     2.393186  \n",
       "2     2.412440  \n",
       "12    2.639031  \n",
       "6     2.675938  \n",
       "1     2.695160  \n",
       "24    2.853394  \n",
       "5     4.931353  \n",
       "11    4.994058  \n",
       "0     5.312609  \n",
       "17    5.583628  \n",
       "23    7.011712  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"history_nb_layers=20.csv\")\n",
    "\n",
    "# Apply multiple filters and sort the DataFrame in one go\n",
    "df_sorted = df[\n",
    "    (df[\"FHE_timing\"] != -1)\n",
    "    & (abs(df[\"Mean_disable_accuracy\"] - df[\"Mean_simulate_accuracy\"]) < 0.4)\n",
    "    & (abs(df[\"Mean_FP32_accuracy\"] - df[\"Mean_simulate_accuracy\"]) < 0.3)\n",
    "].sort_values(by=[\"FHE_timing\", \"Mean_simulate_accuracy\"], ascending=[True, False])\n",
    "\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26.530807676353298, 43.576600653800575)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "233 / 8.782243, 115 / 2.639031\n",
    "\n",
    "traing et composabilité bj 1.2\n",
    "zjouter un pbs av clipping\n",
    "\n",
    "data frame et training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layers = 50\n",
    "bits = 4\n",
    "\n",
    "param[\"dir\"] = f\"./checkpoints/MNIST/NLP_{nb_layers}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 50-nb_layers: acc_after_ft_train=98.027% vs acc_after_ft_test=96.845%\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "checkpoint = torch.load(\n",
    "    f\"checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt\", map_location=device\n",
    ")\n",
    "\n",
    "fp32_mnist = Fp32MNIST(param[\"output_size\"], nb_layers=nb_layers).to(device)\n",
    "\n",
    "fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "acc_after_ft_train = torch_inference(fp32_mnist, train_loader, device=device)\n",
    "acc_after_ft_test = torch_inference(fp32_mnist, test_loader, device=device)\n",
    "\n",
    "print(\n",
    "    f\"With {fp32_mnist.nb_layers}-nb_layers: \"\n",
    "    f\"{acc_after_ft_train=:.3%} vs {acc_after_ft_test=:.3%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/MNIST/NLP_50/quant_mnist_bits=4/MNIST_quant_mnist_bits=4_state_dict.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9352154482390609, 0.9340945512820513)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = (\n",
    "    f\"checkpoints/MNIST/NLP_{nb_layers}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    ")\n",
    "print(path)\n",
    "quant_mnist = QuantMNIST(bit=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "quant_mnist.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "torch_inference(quant_mnist, train_loader, device=device), torch_inference(\n",
    "    quant_mnist, test_loader, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bits in [4]:  # 3\n",
    "\n",
    "    nb_layers = 50\n",
    "    param[\"dir\"] = f\"./checkpoints/MNIST/NLP_{nb_layers}/\"\n",
    "    checkpoint = torch.load(\n",
    "        f\"checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt\", map_location=device\n",
    "    )\n",
    "    fp32_mnist = Fp32MNIST(param[\"output_size\"], nb_layers=nb_layers).to(device)\n",
    "    fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "    path = f\"checkpoints/MNIST/NLP_{nb_layers}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    "    quant_mnist = QuantMNIST(bit=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "    quant_mnist.load_state_dict(torch.load(path, map_location=device))\n",
    "    _ = run(fp32_mnist, quant_mnist, \"ptq\", data_calibration, test_loader, bits=bits)\n",
    "\n",
    "_ = run(fp32_mnist, quant_mnist, \"qat\", data_calibration, test_loader, bits=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_layers=50, bits=4, compile_type='qat', compile_function=<function compile_brevitas_qat_model at 0x7fbb7f5ea3b0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0: max_bits=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [21:12,  8.15s/it]\n",
      "1it [21:30, 1290.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0: row=['qat', 50, 4, 7, 14, 0.8818108974358975, 0.9388020833333334, 0.9388020833333334, 14.862058889865875]\n",
      "j=1: max_bits=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [10:58,  4.22s/it]\n",
      "2it [32:44, 928.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=1: row=['qat', 50, 4, 6, 14, 0.8908253205128205, 0.9111578525641025, 0.9111578525641025, 4.996004994710287]\n",
      "j=2: max_bits=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [08:55,  3.44s/it]\n",
      "3it [41:56, 756.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=2: row=['qat', 50, 4, 5, 15, 0.8833133012820513, 0.8971354166666666, 0.8971354166666666, 3.0782105763753256]\n",
      "j=3: max_bits=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [08:28,  3.26s/it]\n",
      "4it [50:39, 664.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=3: row=['qat', 50, 4, 4, 14, 0.8880208333333334, 0.29767628205128205, 0.29767628205128205, 2.6694800734519957]\n",
      "x.bit_width=10 > lsbs_to_remove=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [50:51, 428.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=4: row=['qat', 50, 4, <Exactness.APPROXIMATE: 1>, -1, -1, -1, -1, -1]\n",
      "x.bit_width=10 > lsbs_to_remove=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [51:03, 510.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=5: row=['qat', 50, 4, <Exactness.EXACT: 0>, -1, -1, -1, -1, -1]\n",
      "nb_layers=50, bits=3, compile_type='ptq', compile_function=<function compile_torch_model at 0x7fbb7f5ea290>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0: max_bits=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [12:15,  4.72s/it]\n",
      "1it [12:29, 749.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0: row=['ptq', 50, 3, 7, 11, 0.9684495192307693, 0.08944310897435898, 0.08914262820512821, 7.475684559345245]\n",
      "j=1: max_bits=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [08:10,  3.14s/it]\n",
      "2it [20:53, 604.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=1: row=['ptq', 50, 3, 6, 11, 0.9659455128205128, 0.14753605769230768, 0.14703525641025642, 3.316960330804189]\n",
      "j=2: max_bits=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [06:34,  2.53s/it]\n",
      "3it [27:40, 514.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=2: row=['ptq', 50, 3, 5, 11, 0.9667467948717948, 0.10176282051282051, 0.10166266025641026, 1.82840656042099]\n",
      "j=3: max_bits=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [06:20,  2.44s/it]\n",
      "4it [34:13, 466.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=3: row=['ptq', 50, 3, 4, 11, 0.9654447115384616, 0.1282051282051282, 0.12830528846153846, 1.4176392356554668]\n",
      "j=4: max_bits=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [06:20,  2.44s/it]\n",
      "5it [40:47, 440.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=4: row=['ptq', 50, 3, <Exactness.APPROXIMATE: 1>, 10, 0.9685496794871795, 0.09815705128205128, 0.09815705128205128, 1.5415482838948569]\n",
      "x.bit_width=7 > lsbs_to_remove=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [40:56, 409.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=5: row=['ptq', 50, 3, <Exactness.EXACT: 0>, -1, -1, -1, -1, -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for bits in [4]:  # 3\n",
    "\n",
    "    nb_layers = 50\n",
    "    param[\"dir\"] = f\"./checkpoints/MNIST/NLP_{nb_layers}/\"\n",
    "    checkpoint = torch.load(\n",
    "        f\"checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt\", map_location=device\n",
    "    )\n",
    "    fp32_mnist = Fp32MNIST(param[\"output_size\"], nb_layers=nb_layers).to(device)\n",
    "    fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "    path = f\"checkpoints/MNIST/NLP_{nb_layers}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    "    quant_mnist = QuantMNIST(bit=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "    quant_mnist.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    history = run(fp32_mnist, quant_mnist, \"qat\", data_calibration, test_loader, bits=None)\n",
    "\n",
    "    history = run(fp32_mnist, quant_mnist, \"ptq\", data_calibration, test_loader, bits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_layers=50, bits=4, compile_type='qat', compile_function=<function compile_brevitas_qat_model at 0x7fbb7f5ea3b0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0: max_bits=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [20:51,  8.02s/it]\n",
      "1it [21:09, 1269.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0: row=['qat', 50, 4, 7, 14, 0.8887219551282052, 0.9387019230769231, 0.9387019230769231, 14.696718458334605]\n",
      "j=1: max_bits=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [10:44,  4.13s/it]\n",
      "2it [32:10, 911.49s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=1: row=['qat', 50, 4, 6, 14, 0.8874198717948718, 0.8811097756410257, 0.8751001602564102, 4.802688153584798]\n",
      "j=2: max_bits=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [09:03,  3.48s/it]\n",
      "3it [41:30, 751.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=2: row=['qat', 50, 4, 5, 14, 0.8928285256410257, 0.8090945512820513, 0.8090945512820513, 3.194890900452932]\n",
      "j=3: max_bits=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [08:34,  3.30s/it]\n",
      "4it [50:23, 664.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=3: row=['qat', 50, 4, 4, 14, 0.8902243589743589, 0.27333733974358976, 0.25951522435897434, 2.738588531812032]\n",
      "x.bit_width=10 > lsbs_to_remove=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [50:35, 429.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=4: row=['qat', 50, 4, <Exactness.APPROXIMATE: 1>, -1, -1, -1, -1, -1]\n",
      "x.bit_width=10 > lsbs_to_remove=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [50:47, 507.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=5: row=['qat', 50, 4, <Exactness.EXACT: 0>, -1, -1, -1, -1, -1]\n",
      "nb_layers=50, bits=5, compile_type='ptq', compile_function=<function compile_torch_model at 0x7fbb7f5ea290>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0: max_bits=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [13:39,  5.26s/it]\n",
      "1it [13:54, 834.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0: row=['ptq', 50, 5, 7, 15, 0.969551282051282, 0.9380008012820513, 0.9354967948717948, 8.782243152459463]\n",
      "j=1: max_bits=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [13:24,  5.16s/it]\n",
      "2it [27:34, 825.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=1: row=['ptq', 50, 5, 6, 15, 0.9636418269230769, 0.9043469551282052, 0.9039463141025641, 8.528247785568237]\n",
      "j=2: max_bits=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [09:20,  3.59s/it]\n",
      "3it [37:07, 710.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=2: row=['ptq', 50, 5, 5, 15, 0.967948717948718, 0.3075921474358974, 0.3075921474358974, 4.473695894082387]\n",
      "j=3: max_bits=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [08:50,  3.40s/it]\n",
      "4it [46:12, 645.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=3: row=['ptq', 50, 5, 4, 15, 0.9688501602564102, 0.17908653846153846, 0.1711738782051282, 3.9235528707504272]\n",
      "j=4: max_bits=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [08:42,  3.35s/it]\n",
      "5it [55:07, 605.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=4: row=['ptq', 50, 5, <Exactness.APPROXIMATE: 1>, 15, 0.9677483974358975, 0.09825721153846154, 0.09825721153846154, 3.812394960721334]\n",
      "j=5: max_bits=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [08:59,  3.46s/it]\n",
      "6it [1:04:20, 643.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=5: row=['ptq', 50, 5, <Exactness.EXACT: 0>, 13, 0.9669471153846154, 0.08914262820512821, 0.08914262820512821, 4.108425911267599]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for bits in [4]:  # 3\n",
    "\n",
    "    nb_layers = 50\n",
    "    param[\"dir\"] = f\"./checkpoints/MNIST/NLP_{nb_layers}/\"\n",
    "    checkpoint = torch.load(\n",
    "        f\"checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt\", map_location=device\n",
    "    )\n",
    "    fp32_mnist = Fp32MNIST(param[\"output_size\"], nb_layers=nb_layers).to(device)\n",
    "    fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "    path = f\"checkpoints/MNIST/NLP_{nb_layers}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    "    quant_mnist = QuantMNIST(bit=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "    quant_mnist.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    history = run(fp32_mnist, quant_mnist, \"qat\", data_calibration, test_loader, bits=None)\n",
    "\n",
    "    history = run(fp32_mnist, quant_mnist, \"ptq\", data_calibration, test_loader, bits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0: max_bits=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "run(fp32_mnist, quant_mnist, \"qat\", data_calibration, test_loader, bits=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in list(fp32_mnist.named_parameters())[:-1]:\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch  0: Train loss = 0.4400 VS Test loss = 0.6064 - Accuracy train: 0.8924 VS Accuracy test: 0.8232\n",
      "Epoch  1: Train loss = 0.4486 VS Test loss = 0.6250 - Accuracy train: 0.8914 VS Accuracy test: 0.7844\n",
      "Epoch  2: Train loss = 0.4293 VS Test loss = 0.6240 - Accuracy train: 0.8962 VS Accuracy test: 0.7443\n",
      " 30%|███       | 3/10 [05:24<12:37, 108.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilestones\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m---> 10\u001b[0m quant_mnist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_mnist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/use_case_examples/cifar/cifar_brevitas_finetuning/cifar_utils.py:414\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, param, step, device)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m    413\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 414\u001b[0m     yhat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m    415\u001b[0m     loss_test \u001b[39m=\u001b[39m param[\u001b[39m\"\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m\"\u001b[39m](yhat, y)\n\u001b[1;32m    416\u001b[0m     loss_batch_test\u001b[39m.\u001b[39mappend(loss_test\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 75\u001b[0m, in \u001b[0;36mQuantMNIST.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midentity2(x)\n\u001b[0;32m---> 75\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinears\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:143\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_input_dim(\u001b[39minput\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     exponential_average_factor \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "if run:\n",
    "    param[\"training\"] = f\"quant_mnist_{bits=}\"\n",
    "    param[\"epochs\"] = 30\n",
    "    param[\"lr\"] = 0.1\n",
    "    param[\"milestones\"] = [3, 5, 10, 20]\n",
    "    param[\"gamma\"] = 0.1\n",
    "\n",
    "    quant_mnist = train(quant_mnist.to(device), train_loader, test_loader, param, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09705528846153846"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_mnist = QuantMNIST(bit=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "\n",
    "quant_mnist = mapping_keys(checkpoint, quant_mnist, device=\"cpu\")\n",
    "acc_before_ft = torch_inference(quant_mnist, test_loader, device=device)\n",
    "\n",
    "acc_before_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/MNIST/NLP_50/quant_mnist_bits=4/MNIST_quant_mnist_bits=4_state_dict.pt\n",
      "With 4-bits and 50-nb_layers: acc_after_ft_train=73.681% vs acc_after_ft_test=73.578%\n",
      "Maximum bit-width in the circuit: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = (\n",
    "    f\"checkpoints/MNIST/NLP_{nb_layers}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    ")\n",
    "print(path)\n",
    "quant_mnist = QuantMNIST(bit=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "quant_mnist.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "acc_after_ft_train = torch_inference(quant_mnist, train_loader, device=device)\n",
    "acc_after_ft_test = torch_inference(quant_mnist, test_loader, device=device)\n",
    "\n",
    "print(\n",
    "    f\"With {quant_mnist.bit}-bits and {quant_mnist.nb_layers}-nb_layers: \"\n",
    "    f\"{acc_after_ft_train=:.3%} vs {acc_after_ft_test=:.3%}\"\n",
    ")\n",
    "\n",
    "qmodel = fhe_compatibility(quant_mnist, data_calibration, rounding_threshold_bits=bits)\n",
    "print(f\"Maximum bit-width in the circuit: {qmodel.fhe_circuit.graph.maximum_integer_bit_width()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: Train loss = 0.4610 VS Test loss = 0.6144 - Accuracy train: 0.8876 VS Accuracy test: 0.7494\n",
      "Epoch  1: Train loss = 0.4565 VS Test loss = 0.5531 - Accuracy train: 0.8889 VS Accuracy test: 0.8294\n",
      "Epoch  2: Train loss = 0.4495 VS Test loss = 0.4926 - Accuracy train: 0.8930 VS Accuracy test: 0.8483\n",
      "Epoch  3: Train loss = 0.4567 VS Test loss = 0.6339 - Accuracy train: 0.8899 VS Accuracy test: 0.8384\n",
      "Epoch  4: Train loss = 0.4601 VS Test loss = 0.8759 - Accuracy train: 0.8886 VS Accuracy test: 0.7358\n",
      "100%|██████████| 5/5 [07:12<00:00, 86.40s/it]\n",
      "Save in: checkpoints/MNIST/NLP_50/quant_mnist_bits=4/MNIST_quant_mnist_bits=4_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "# if run:\n",
    "#     param[\"training\"] = f\"quant_mnist_{bits=}\"\n",
    "#     param[\"epochs\"] = 5\n",
    "#     param[\"lr\"] = 0.1\n",
    "#     param[\"milestones\"] = [1, 3]\n",
    "#     param[\"gamma\"] = 0.1\n",
    "\n",
    "#     quant_mnist_20 = train(quant_mnist_20, train_loader, test_loader, param, device=device)\n",
    "\n",
    "\n",
    "# for n, p in list(fp32_mnist.named_parameters())[:-1]:\n",
    "#         p.requires_grad = False\n",
    "\n",
    "assert str(quant_mnist.nb_layers) in param[\"dir\"]\n",
    "\n",
    "if run:\n",
    "    param[\"training\"] = f\"quant_mnist_{bits=}\"\n",
    "    param[\"epochs\"] = 5\n",
    "    param[\"lr\"] = 0.00000001\n",
    "    param[\"milestones\"] = []\n",
    "    param[\"gamma\"] = 0.1\n",
    "    quant_mnist = train(quant_mnist, train_loader, test_loader, param, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 50-nb_layers: acc_after_ft_train=98.012% vs acc_after_ft_test=96.855%\n"
     ]
    }
   ],
   "source": [
    "nb_layers = 50\n",
    "\n",
    "param[\"dir\"] = f\"./checkpoints/MNIST/NLP_{nb_layers}/\"\n",
    "\n",
    "fp32_mnist = Fp32MNIST(param[\"output_size\"], nb_layers=nb_layers).to(device)\n",
    "checkpoint = torch.load(\n",
    "    f\"checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt\", map_location=device\n",
    ")\n",
    "fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "acc_after_ft_train = torch_inference(fp32_mnist, train_loader, device=device)\n",
    "acc_after_ft_test = torch_inference(fp32_mnist, test_loader, device=device)\n",
    "\n",
    "print(\n",
    "    f\"With {fp32_mnist.nb_layers}-nb_layers: \"\n",
    "    f\"{acc_after_ft_train=:.3%} vs {acc_after_ft_test=:.3%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before fine-tuning: acc_before_ft=9.696%\n",
      "Maximum bit-width in the circuit: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bits, run, nb_layers = 4, True, 50\n",
    "\n",
    "quant_mnist = QuantMNIST(bit=bits, output_size=param[\"output_size\"], nb_layers=nb_layers).to(\"cpu\")\n",
    "quant_mnist = mapping_keys(checkpoint, quant_mnist, device=\"cpu\")\n",
    "acc_before_ft = torch_inference(quant_mnist, test_loader, device=device)\n",
    "\n",
    "\"\"\"\n",
    "Accuracy before fine-tuning: acc_before_ft=10.286%\n",
    "Maximum bit-width in the circuit: 16\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Accuracy before fine-tuning: {acc_before_ft=:.3%}\")\n",
    "qmodel = fhe_compatibility(quant_mnist, data_calibration, rounding_threshold_bits=bits)\n",
    "print(f\"Maximum bit-width in the circuit: {qmodel.fhe_circuit.graph.maximum_integer_bit_width()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/MNIST/NLP_50/quant_mnist_bits=4/MNIST_quant_mnist_bits=4_state_dict.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = (\n",
    "    f\"checkpoints/MNIST/NLP_{nb_layers}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    ")\n",
    "print(path)\n",
    "quant_mnist = QuantMNIST(bit=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "quant_mnist.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "# acc_after_ft_train = torch_inference(quant_mnist, train_loader, device=device)\n",
    "# acc_after_ft_test  = torch_inference(quant_mnist, test_loader, device=device)\n",
    "\n",
    "# print(\n",
    "#     f\"With {quant_mnist.bit}-bits and {quant_mnist.nb_layers}-nb_layers: \"\n",
    "#     f\"{acc_after_ft_train=:.3%} vs {acc_after_ft_test=:.3%}\"\n",
    "#     )\n",
    "\n",
    "# qmodel = fhe_compatibility(quant_mnist, data_calibration, rounding_threshold_bits=bits)\n",
    "# print(f\"Maximum bit-width in the circuit: {qmodel.fhe_circuit.graph.maximum_integer_bit_width()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: Train loss = 0.4400 VS Test loss = 0.6100 - Accuracy train: 0.8940 VS Accuracy test: 0.8411\n",
      "Epoch  1: Train loss = 0.4530 VS Test loss = 0.3802 - Accuracy train: 0.8917 VS Accuracy test: 0.9366\n",
      "Epoch  2: Train loss = 0.4523 VS Test loss = 0.5532 - Accuracy train: 0.8917 VS Accuracy test: 0.7930\n",
      "Epoch  3: Train loss = 0.4462 VS Test loss = 0.4610 - Accuracy train: 0.8922 VS Accuracy test: 0.9114\n",
      "Epoch  4: Train loss = 0.4488 VS Test loss = 0.5657 - Accuracy train: 0.8912 VS Accuracy test: 0.7489\n",
      "Epoch  5: Train loss = 0.4541 VS Test loss = 0.4020 - Accuracy train: 0.8917 VS Accuracy test: 0.9104\n",
      "Epoch  6: Train loss = 0.4468 VS Test loss = 0.5325 - Accuracy train: 0.8920 VS Accuracy test: 0.8685\n",
      "Epoch  7: Train loss = 0.4451 VS Test loss = 0.4936 - Accuracy train: 0.8929 VS Accuracy test: 0.8282\n",
      "Epoch  8: Train loss = 0.4484 VS Test loss = 0.5095 - Accuracy train: 0.8922 VS Accuracy test: 0.8144\n",
      "Epoch  9: Train loss = 0.4426 VS Test loss = 0.5891 - Accuracy train: 0.8925 VS Accuracy test: 0.7720\n",
      "Epoch 10: Train loss = 0.4487 VS Test loss = 0.4339 - Accuracy train: 0.8924 VS Accuracy test: 0.9341\n",
      "Epoch 11: Train loss = 0.4450 VS Test loss = 0.5444 - Accuracy train: 0.8921 VS Accuracy test: 0.8189\n",
      "Epoch 12: Train loss = 0.4442 VS Test loss = 0.5472 - Accuracy train: 0.8927 VS Accuracy test: 0.8343\n",
      "Epoch 13: Train loss = 0.4475 VS Test loss = 0.4131 - Accuracy train: 0.8922 VS Accuracy test: 0.9317\n",
      "Epoch 14: Train loss = 0.4479 VS Test loss = 0.5178 - Accuracy train: 0.8931 VS Accuracy test: 0.8151\n",
      "100%|██████████| 15/15 [22:17<00:00, 89.16s/it]\n",
      "Save in: checkpoints/MNIST/NLP_50/quant_mnist_bits=4/MNIST_quant_mnist_bits=4_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "assert str(quant_mnist.nb_layers) in param[\"dir\"]\n",
    "\n",
    "if run:\n",
    "    param[\"training\"] = f\"quant_mnist_{bits=}\"\n",
    "    param[\"epochs\"] = 5\n",
    "    param[\"lr\"] = 0.000001\n",
    "    param[\"milestones\"] = []\n",
    "    param[\"gamma\"] = 0.1\n",
    "    quant_mnist = train(quant_mnist, train_loader, test_loader, param, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum bit-width in the circuit: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel = fhe_compatibility(quant_mnist, data_calibration, rounding_threshold_bits=bits)\n",
    "print(f\"Maximum bit-width in the circuit: {qmodel.fhe_circuit.graph.maximum_integer_bit_width()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layers = 100\n",
    "\n",
    "param[\"dir\"] = f\"./checkpoints/MNIST/NLP_{nb_layers}/\"\n",
    "\n",
    "fp32_mnist = Fp32MNIST(nb_layers=nb_layers).to(device)\n",
    "\n",
    "# checkpoint = torch.load(f\"checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt\", map_location=device)\n",
    "\n",
    "# fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "# acc_after_ft_train = torch_inference(fp32_mnist, train_loader, device=device)\n",
    "# acc_after_ft_test  = torch_inference(fp32_mnist, test_loader, device=device)\n",
    "\n",
    "# print(\n",
    "#     f\"With {fp32_mnist.nb_layers}-nb_layers: \"\n",
    "#     f\"{acc_after_ft_train=:.3%} vs {acc_after_ft_test=:.3%}\"\n",
    "#     )\n",
    "\n",
    "\n",
    "for n, p in list(fp32_mnist.named_parameters()):\n",
    "    p.requires_grad = False\n",
    "\n",
    "# for n, p in list(fp32_mnist.named_parameters())[0:2]:\n",
    "#         p.requires_grad = True\n",
    "\n",
    "for n, p in list(fp32_mnist.named_parameters())[-1:]:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: Train loss = 2.4618 VS Test loss = 6525.8071 - Accuracy train: 0.0978 VS Accuracy test: 0.1087\n",
      "Epoch  1: Train loss = 2.4637 VS Test loss = 5.4402 - Accuracy train: 0.1001 VS Accuracy test: 0.0889\n",
      "Epoch  2: Train loss = 2.4636 VS Test loss = 5.0099 - Accuracy train: 0.0984 VS Accuracy test: 0.0976\n",
      "Epoch  3: Train loss = nan VS Test loss = nan - Accuracy train: 0.0974 VS Accuracy test: 0.0980\n",
      "Epoch  4: Train loss = nan VS Test loss = nan - Accuracy train: 0.0987 VS Accuracy test: 0.0981\n",
      "Epoch  5: Train loss = nan VS Test loss = nan - Accuracy train: 0.0988 VS Accuracy test: 0.0982\n",
      "Epoch  6: Train loss = nan VS Test loss = nan - Accuracy train: 0.0988 VS Accuracy test: 0.0982\n",
      " 35%|███▌      | 7/20 [06:05<11:18, 52.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilestones\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m15\u001b[39m]\n\u001b[1;32m     20\u001b[0m param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m---> 22\u001b[0m fp32_mnist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp32_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/use_case_examples/cifar/cifar_brevitas_finetuning/cifar_utils.py:387\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, param, step, device)\u001b[0m\n\u001b[1;32m    384\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    385\u001b[0m loss_batch_train, accuracy_batch_train \u001b[39m=\u001b[39m [], []\n\u001b[0;32m--> 387\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m    388\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    390\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1799\u001b[0m, in \u001b[0;36mGaussianBlur.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m \u001b[39m    img (PIL Image or Tensor): image to be blurred.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[39m    PIL Image or Tensor: Gaussian blurred image\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m sigma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma[\u001b[39m1\u001b[39m])\n\u001b[0;32m-> 1799\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgaussian_blur(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, [sigma, sigma])\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1366\u001b[0m, in \u001b[0;36mgaussian_blur\u001b[0;34m(img, kernel_size, sigma)\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be PIL Image or Tensor. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1364\u001b[0m     t_img \u001b[39m=\u001b[39m pil_to_tensor(img)\n\u001b[0;32m-> 1366\u001b[0m output \u001b[39m=\u001b[39m F_t\u001b[39m.\u001b[39;49mgaussian_blur(t_img, kernel_size, sigma)\n\u001b[1;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m   1369\u001b[0m     output \u001b[39m=\u001b[39m to_pil_image(output, mode\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39mmode)\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:755\u001b[0m, in \u001b[0;36mgaussian_blur\u001b[0;34m(img, kernel_size, sigma)\u001b[0m\n\u001b[1;32m    752\u001b[0m _assert_image_tensor(img)\n\u001b[1;32m    754\u001b[0m dtype \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mdtype \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_floating_point(img) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mfloat32\n\u001b[0;32m--> 755\u001b[0m kernel \u001b[39m=\u001b[39m _get_gaussian_kernel2d(kernel_size, sigma, dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mimg\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    756\u001b[0m kernel \u001b[39m=\u001b[39m kernel\u001b[39m.\u001b[39mexpand(img\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m], \u001b[39m1\u001b[39m, kernel\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], kernel\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m    758\u001b[0m img, need_cast, need_squeeze, out_dtype \u001b[39m=\u001b[39m _cast_squeeze_in(\n\u001b[1;32m    759\u001b[0m     img,\n\u001b[1;32m    760\u001b[0m     [\n\u001b[1;32m    761\u001b[0m         kernel\u001b[39m.\u001b[39mdtype,\n\u001b[1;32m    762\u001b[0m     ],\n\u001b[1;32m    763\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:742\u001b[0m, in \u001b[0;36m_get_gaussian_kernel2d\u001b[0;34m(kernel_size, sigma, dtype, device)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_gaussian_kernel2d\u001b[39m(\n\u001b[1;32m    740\u001b[0m     kernel_size: List[\u001b[39mint\u001b[39m], sigma: List[\u001b[39mfloat\u001b[39m], dtype: torch\u001b[39m.\u001b[39mdtype, device: torch\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    741\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 742\u001b[0m     kernel1d_x \u001b[39m=\u001b[39m _get_gaussian_kernel1d(kernel_size[\u001b[39m0\u001b[39;49m], sigma[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39mto(device, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    743\u001b[0m     kernel1d_y \u001b[39m=\u001b[39m _get_gaussian_kernel1d(kernel_size[\u001b[39m1\u001b[39m], sigma[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    744\u001b[0m     kernel2d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(kernel1d_y[:, \u001b[39mNone\u001b[39;00m], kernel1d_x[\u001b[39mNone\u001b[39;00m, :])\n",
      "File \u001b[0;32m~/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:733\u001b[0m, in \u001b[0;36m_get_gaussian_kernel1d\u001b[0;34m(kernel_size, sigma)\u001b[0m\n\u001b[1;32m    730\u001b[0m ksize_half \u001b[39m=\u001b[39m (kernel_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m    732\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinspace(\u001b[39m-\u001b[39mksize_half, ksize_half, steps\u001b[39m=\u001b[39mkernel_size)\n\u001b[0;32m--> 733\u001b[0m pdf \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mexp(\u001b[39m-\u001b[39;49m\u001b[39m0.5\u001b[39;49m \u001b[39m*\u001b[39;49m (x \u001b[39m/\u001b[39;49m sigma)\u001b[39m.\u001b[39;49mpow(\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    734\u001b[0m kernel1d \u001b[39m=\u001b[39m pdf \u001b[39m/\u001b[39m pdf\u001b[39m.\u001b[39msum()\n\u001b[1;32m    736\u001b[0m \u001b[39mreturn\u001b[39;00m kernel1d\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for name in fp32_mnist:\n",
    "#     if name in mnist_nlp50 and fp32_mnist[name].size() == mnist_nlp50[name].size():\n",
    "#         mnist_nlp50[name].copy_(fp32_mnist[name])\n",
    "\n",
    "# fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "for n, p in list(fp32_mnist.named_parameters())[0:95]:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# for n, p in list(fp32_mnist.named_parameters())[]:\n",
    "#         p.requires_grad = True\n",
    "\n",
    "run = 1\n",
    "\n",
    "if run:\n",
    "    param[\"training\"] = \"fp32\"\n",
    "    param[\"epochs\"] = 20\n",
    "    param[\"lr\"] = 0.1\n",
    "    param[\"milestones\"] = [4, 15]\n",
    "    param[\"gamma\"] = 0.1\n",
    "\n",
    "    fp32_mnist = train(fp32_mnist, train_loader, test_loader, param, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1069ca3ee43010eb0dc75ae1023d43c37aff1ecf2400458a3da60effb006872e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
