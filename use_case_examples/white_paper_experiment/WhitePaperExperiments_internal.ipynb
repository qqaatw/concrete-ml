{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing Paper **Programmable Bootstrapping Enables Efficient Homomorphic Inference of Deep Neural Networks** : Benchmarking Results\n",
    "\n",
    "This notebook replicates experiments from the paper [_Programmable Bootstrapping Enables Efficient Homomorphic Inference of Deep Neural Networks_](https://whitepaper.zama.ai/), published in 2021. \n",
    "It provides an in-depth analysis of the deep neural network architectures NN-20 and NN-50, along with their training processes using floating point precision and their [quantization](https://docs.zama.ai/concrete-ml/explanations/quantization) using the Quantization Aware Training (QAT) and Post Training Quantization (PTQ) methods. \n",
    "\n",
    "We compare the original paper's findings with the results from the latest version of [Concrete ML](https://pypi.org/project/concrete-ml/). This comparison highlights the significant advancements made by Concrete ML, particularly in execution speed while preserving model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from glob import glob\n",
    "from typing import Callable, List\n",
    "\n",
    "import brevitas\n",
    "import brevitas.nn as qnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from utils_experiments import MEAN, STD, mapping_keys, plot_dataset, torch_inference, train\n",
    "\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model, compile_torch_model\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "All networks begin with a convolutional layer configured with `in_channel=1, out_channels=1, kernel_size=3, stride=1, padding_mode='replicate'`. \n",
    "\n",
    "This is followed by 20 linear layers of 92 neurones with ReLU activation for NN-20, and 50 layers for NN-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"  # torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Input size, 28x28 pixels, a standard size for MNIST images\n",
    "INPUT_IMG_SIZE = 28\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Seed to ensure reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_MAPS = [\n",
    "    # The Bravitas.QuantIdentiy layer, only used in the quant NN, aims to quantize the input.\n",
    "    # (\"I\",),\n",
    "    # Convolution layer, with:\n",
    "    # in_channel=1, out_channels=1, kernel_size=3, stride=1, padding_mode='replicate'\n",
    "    (\"C\", 1, 1, 3, 1, \"replicate\"),\n",
    "]\n",
    "\n",
    "\n",
    "# The article presents 3 neural network depths. In this notebook, we focus NN-20 and NN-50\n",
    "# architectures. The parameter `nb_layers`: controls the depth of the NN.\n",
    "def LINEAR_LAYERS(nb_layers: int, output_size: int):\n",
    "    return (  # noqa: W503\n",
    "        [\n",
    "            (\"R\",),\n",
    "            (\"L\", INPUT_IMG_SIZE * INPUT_IMG_SIZE, 92),\n",
    "            (\"B\", 92),\n",
    "        ]  # noqa: W503\n",
    "        + [  # noqa: W503\n",
    "            (\"R\",),\n",
    "            (\"L\", 92, 92),\n",
    "            (\"B\", 92),\n",
    "        ]\n",
    "        * (nb_layers - 2)  # noqa: W503\n",
    "        + [(\"L\", 92, output_size)]  # noqa: W503\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 MNIST Neural Nerwork "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fp32MNIST(torch.nn.Module):\n",
    "    \"\"\"MNIST Torch model.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_layers: int, output_size: int = 10):\n",
    "        super(Fp32MNIST, self).__init__()\n",
    "        \"\"\"MNIST Torch model.\n",
    "\n",
    "        Args:\n",
    "            nb_layers (int): Number of layers.\n",
    "            output_size (int): Number of classes.\n",
    "        \"\"\"\n",
    "        self.nb_layers = nb_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        def make_layers(t):\n",
    "            if t[0] == \"C\":\n",
    "                # Workaround: stride=1, padding_mode='replicate' is replaced by\n",
    "                # transforms.Pad(1, padding_mode=\"edge\")\n",
    "                return torch.nn.Conv2d(\n",
    "                    in_channels=t[1],\n",
    "                    out_channels=t[2],\n",
    "                    kernel_size=t[3],\n",
    "                )\n",
    "            if t[0] == \"L\":\n",
    "                return torch.nn.Linear(in_features=t[1], out_features=t[2])\n",
    "            if t[0] == \"R\":\n",
    "                return torch.nn.ReLU()\n",
    "            if t[0]:\n",
    "                return torch.nn.BatchNorm1d(t[1])\n",
    "\n",
    "            raise NameError(f\"'{t}' not defined\")\n",
    "\n",
    "        # QuantIdentity layers are ignored in the floationg point architecture.\n",
    "        self.features_maps = torch.nn.Sequential(\n",
    "            *[make_layers(t) for t in FEATURES_MAPS if t[0] != \"I\"]\n",
    "        )\n",
    "        self.linears = torch.nn.Sequential(\n",
    "            *[\n",
    "                make_layers(t)\n",
    "                for t in LINEAR_LAYERS(self.nb_layers, self.output_size)\n",
    "                if t[0] != \"I\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_maps(x)\n",
    "        x = torch.nn.Flatten()(x)\n",
    "        x = self.linears(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized MNIST Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantMNIST(torch.nn.Module):\n",
    "    \"\"\"Quantized MNIST network with Brevitas.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_bits: int,\n",
    "        nb_layers: int,\n",
    "        output_size: int = 10,\n",
    "        act_quant: brevitas.quant = brevitas.quant.Int8ActPerTensorFloat,\n",
    "        weight_quant: brevitas.quant = brevitas.quant.Int8WeightPerTensorFloat,\n",
    "    ):\n",
    "        \"\"\"A quantized network with Brevitas.\n",
    "\n",
    "        Args:\n",
    "            n_bits (int): Precision bit of quantization.\n",
    "            nb_layers (int): Number of layers.\n",
    "            output_size (int): Number of classes\n",
    "            act_quant (brevitas.quant): Quantization protocol of activations.\n",
    "            weight_quant (brevitas.quant): Quantization protocol of the weights.\n",
    "        \"\"\"\n",
    "        super(QuantMNIST, self).__init__()\n",
    "\n",
    "        self.n_bits = n_bits\n",
    "        self.nb_layers = nb_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        def tuple2quantlayer(t):\n",
    "            if t[0] == \"R\":\n",
    "                return qnn.QuantReLU(\n",
    "                    return_quant_tensor=True, bit_width=n_bits, act_quant=act_quant\n",
    "                )\n",
    "            if t[0] == \"C\":\n",
    "                # Workaround: stride=1, padding_mode='replicate' is replaced by\n",
    "                # transforms.Pad(1, padding_mode=\"edge\")\n",
    "                return qnn.QuantConv2d(\n",
    "                    t[1],\n",
    "                    t[2],\n",
    "                    kernel_size=t[3],\n",
    "                    weight_bit_width=2,\n",
    "                    weight_quant=weight_quant,\n",
    "                    return_quant_tensor=True,\n",
    "                )\n",
    "            if t[0] == \"L\":\n",
    "                return qnn.QuantLinear(\n",
    "                    in_features=t[1],\n",
    "                    out_features=t[2],\n",
    "                    weight_bit_width=n_bits,\n",
    "                    weight_quant=weight_quant,\n",
    "                    bias=True,\n",
    "                    return_quant_tensor=True,\n",
    "                )\n",
    "            if t[0] == \"I\":\n",
    "                identity_quant = t[1] if len(t) == 2 else n_bits\n",
    "                return qnn.QuantIdentity(\n",
    "                    bit_width=identity_quant, act_quant=act_quant, return_quant_tensor=True\n",
    "                )\n",
    "            if t[0] == \"B\":\n",
    "                return torch.nn.BatchNorm1d(t[1])\n",
    "\n",
    "        self.features_maps = torch.nn.Sequential(\n",
    "            *[tuple2quantlayer(t) for t in FEATURES_MAPS if t[0]]\n",
    "        )\n",
    "\n",
    "        # self.identity1 and self.identity2 are used to encapsulate the `torch.flatten`.\n",
    "        self.identity1 = qnn.QuantIdentity(\n",
    "            bit_width=n_bits, act_quant=act_quant, return_quant_tensor=True\n",
    "        )\n",
    "        self.identity2 = qnn.QuantIdentity(\n",
    "            bit_width=n_bits, act_quant=act_quant, return_quant_tensor=True\n",
    "        )\n",
    "\n",
    "        # QuantIdentity layers are taken into account\n",
    "        self.linears = torch.nn.Sequential(\n",
    "            *[tuple2quantlayer(t) for t in LINEAR_LAYERS(self.nb_layers, self.output_size)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_maps(x)\n",
    "        x = self.identity1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.identity2(x)\n",
    "        x = self.linears(x)\n",
    "        return x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(\n",
    "    model: Callable,\n",
    "    compile_function: Callable,\n",
    "    compile_type: str,\n",
    "    data_calibration,\n",
    "    data_loader,\n",
    "    n_bits: int,\n",
    "):\n",
    "    nb_layers = model.nb_layers\n",
    "\n",
    "    history = []\n",
    "    filename = f\"./benchmark/history_{nb_layers=}.csv\"\n",
    "\n",
    "    headers = [\n",
    "        \"compile_type\",\n",
    "        \"number_of_layers\",\n",
    "        \"QAT/PTQ_n_bits\",\n",
    "        \"threshold_n_bits\",\n",
    "        \"threshold_method\",\n",
    "        \"max_bits\",\n",
    "        \"mean_FP32_accuracy\",\n",
    "        \"mean_disable_accuracy\",\n",
    "        \"mean_simulate_accuracy\",\n",
    "        \"FHE_timing\",\n",
    "        \"INPUT_COMPRESSION\",\n",
    "    ]\n",
    "\n",
    "    thresholds = [\n",
    "        {\"n_bits\": 5, \"method\": \"APPROXIMATE\"},\n",
    "        {\"n_bits\": 5, \"method\": \"EXACT\"},\n",
    "        {\"n_bits\": 6, \"method\": \"APPROXIMATE\"},\n",
    "        {\"n_bits\": 6, \"method\": \"EXACT\"},\n",
    "        {\"n_bits\": 7, \"method\": \"APPROXIMATE\"},\n",
    "        {\"n_bits\": 7, \"method\": \"EXACT\"},\n",
    "    ]\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(headers)\n",
    "\n",
    "    with open(filename, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for j, threshold in enumerate(thresholds):\n",
    "\n",
    "            fhe_timing = -1\n",
    "            history_fp32_predictions = []\n",
    "            history_disable_predictions = []\n",
    "            history_simulat_predictions = []\n",
    "\n",
    "            try:\n",
    "                q_module = compile_function(\n",
    "                    model.to(DEVICE),\n",
    "                    torch_inputset=data_calibration,\n",
    "                    n_bits=n_bits if compile_type == \"PTQ\" else None,\n",
    "                    rounding_threshold_bits=threshold,\n",
    "                )\n",
    "\n",
    "                max_bits = q_module.fhe_circuit.graph.maximum_integer_bit_width()\n",
    "                print(f\"{j=}: {max_bits=} after compilation\")\n",
    "\n",
    "                for i, (data, labels) in enumerate(data_loader):\n",
    "\n",
    "                    fp32_predictions = model(data).cpu().detach()\n",
    "                    history_fp32_predictions.extend(fp32_predictions.argmax(1) == labels)\n",
    "\n",
    "                    data, labels = data.detach().cpu().numpy(), labels.detach().cpu().numpy()\n",
    "\n",
    "                    disable_predictions = q_module.forward(data, fhe=\"disable\")\n",
    "                    history_disable_predictions.extend(disable_predictions.argmax(1) == labels)\n",
    "\n",
    "                    simulat_predictions = q_module.forward(data, fhe=\"simulate\")\n",
    "                    history_simulat_predictions.extend(simulat_predictions.argmax(1) == labels)\n",
    "\n",
    "                    if i == 0:\n",
    "                        start_time = time.time()\n",
    "                        q_module.forward(data[0, None], fhe=\"execute\")\n",
    "                        fhe_timing = (time.time() - start_time) / 60.0\n",
    "\n",
    "                row = [\n",
    "                    compile_type,\n",
    "                    nb_layers,\n",
    "                    n_bits,\n",
    "                    threshold[\"n_bits\"],\n",
    "                    threshold[\"method\"],\n",
    "                    max_bits,\n",
    "                    np.mean(history_fp32_predictions),\n",
    "                    np.mean(history_disable_predictions),\n",
    "                    np.mean(history_simulat_predictions),\n",
    "                    fhe_timing,\n",
    "                    os.environ.get(\"USE_INPUT_COMPRESSION\", \"1\"),\n",
    "                ]\n",
    "\n",
    "            except BaseException:\n",
    "                row = [\n",
    "                    compile_type,\n",
    "                    nb_layers,\n",
    "                    n_bits,\n",
    "                    threshold[\"n_bits\"],\n",
    "                    threshold[\"method\"],\n",
    "                    -1,\n",
    "                    -1,\n",
    "                    -1,\n",
    "                    -1,\n",
    "                    -1,\n",
    "                    -1,\n",
    "                ]\n",
    "\n",
    "            writer.writerow(row)\n",
    "            print(f\"{j=}: {row=}\")\n",
    "            history.append(dict(zip(headers, row)))\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def ptq_benchmark(\n",
    "    nb_layers: int,\n",
    "    data_calibration: datasets,\n",
    "    data_loader: DataLoader,\n",
    "    n_bits: List = [5, 6, 7],\n",
    "    model_dir: str = \"./checkpoints/MNIST/\",\n",
    "):\n",
    "    # Test all fp32 pre-trained models, saved in the following path\n",
    "    state_dict_paths = glob(f\"{model_dir}/NLP_{nb_layers}/fp32/*fp32_state_dict.pt\")\n",
    "    assert len(state_dict_paths) > 0, f\"There are no fp32 pre-trained models inside {model_dir}.\"\n",
    "\n",
    "    for state_dict_path in state_dict_paths:\n",
    "        checkpoint = torch.load(state_dict_path, map_location=DEVICE)\n",
    "        fp32_model = Fp32MNIST(nb_layers=nb_layers).to(DEVICE)\n",
    "        fp32_model.load_state_dict(checkpoint)\n",
    "        print(f\"FP32 model loaded in: '{state_dict_path}'\")\n",
    "\n",
    "        # In PTQ, we have to specify the quantization bit precision\n",
    "        for bit in tqdm(n_bits):\n",
    "            run_benchmark(\n",
    "                fp32_model,\n",
    "                compile_function=compile_torch_model,\n",
    "                compile_type=\"PTQ\",\n",
    "                data_calibration=data_calibration,\n",
    "                data_loader=data_loader,\n",
    "                n_bits=bit,\n",
    "            )\n",
    "\n",
    "\n",
    "def qat_benchmark(\n",
    "    nb_layers: int,\n",
    "    data_calibration: datasets,\n",
    "    data_loader: DataLoader,\n",
    "    model_dir: str = \"./checkpoints/MNIST\",\n",
    "):\n",
    "    # Test all quantized pre-trained models, saved in the following path\n",
    "    state_dict_paths = glob(f\"{model_dir}/NLP_{nb_layers}/quant_*/*_state_dict.pt\")\n",
    "    assert len(state_dict_paths) > 0, f\"There are no pretrained models in {model_dir}\"\n",
    "\n",
    "    for state_dict_path in state_dict_paths:\n",
    "        checkpoint = torch.load(state_dict_path, map_location=DEVICE)\n",
    "        bit = int(state_dict_path.split(\"/\")[-2].split(\"=\")[-1])\n",
    "        quant_model = QuantMNIST(n_bits=bit, nb_layers=nb_layers).to(DEVICE)\n",
    "        quant_model.load_state_dict(checkpoint)\n",
    "        print(f\"Quantized model loaded in: '{state_dict_path}'\")\n",
    "\n",
    "        run_benchmark(\n",
    "            quant_model,\n",
    "            compile_function=compile_brevitas_qat_model,\n",
    "            compile_type=\"QAT\",\n",
    "            data_calibration=data_calibration,\n",
    "            data_loader=data_loader,\n",
    "            n_bits=bit,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_NOTES_20 = [20, 115.52, 0.97]\n",
    "PAPER_NOTES_50 = [50, 233.55, 0.80]\n",
    "\n",
    "\n",
    "def best_configuration(machine_type, paper_notes):\n",
    "\n",
    "    df = pd.read_csv(f\"benchmark/history_nb_layers={paper_notes[0]}.csv\")\n",
    "    # assert (df[\"mean_disable_accuracy\"] - df[\"mean_simulate_accuracy\"]).mean() <= 0.0005\n",
    "\n",
    "    # Filter data for specific machine\n",
    "    df = df[df[\"machine\"] == machine_type]\n",
    "\n",
    "    # Filter best configuration for specific machine\n",
    "    df = df[\n",
    "        (\n",
    "            (df[\"mean_FP32_accuracy\"] <= df[\"mean_disable_accuracy\"])\n",
    "            | ((df[\"mean_FP32_accuracy\"] - df[\"mean_disable_accuracy\"]) <= 0.01)\n",
    "        )  # nbqa: W503\n",
    "        & (df[\"mean_FP32_accuracy\"] >= paper_notes[2])  # nbqa: W503\n",
    "    ]\n",
    "\n",
    "    best_df = []\n",
    "    for compile_type in [\"PTQ\", \"QAT\"]:\n",
    "\n",
    "        df_filtred = df[df[\"compile_type\"] == compile_type]\n",
    "        df_filtred = df_filtred.sort_values(\n",
    "            by=[\"FHE_timing\", \"mean_simulate_accuracy\"], ascending=[True, False]\n",
    "        )\n",
    "\n",
    "        # Best configuration\n",
    "        best_conf = df_filtred.iloc[0].to_dict()\n",
    "        print(f\"Best configuration with {compile_type}:\\n{best_conf}\")\n",
    "\n",
    "        print(\n",
    "            f\"For machie: {machine_type} - compile_ method : {compile_type}\\n\"\n",
    "            f\"Compared to the data from the paper, which recorded for NN-{paper_notes[0]} and \"\n",
    "            f\"inference time of {paper_notes[1]} seconds.\\nWe observe a significant gain in this new \"\n",
    "            f\"Concrete ML version. The inference time has been reduced to \"\n",
    "            f\"{best_conf['FHE_timing']:.2f} seconds.\\nThis represents a reduction factor of \"\n",
    "            f\"{round(paper_notes[1] / best_conf['FHE_timing'])}.\\n\"\n",
    "        )\n",
    "\n",
    "        best_df.append(df_filtred.head(8))\n",
    "\n",
    "    return pd.concat([best_df[0], best_df[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data-set\n",
    "\n",
    "At the time of writing this notebook, `padding=1` is not yet supported by Concrete ML ; as a workaround, padding is added during the data loading transformation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [  # Workaround: stride=1, padding_mode='replicate' is replaced by\n",
    "        # transforms.Pad(1, padding_mode=\"edge\")\n",
    "        transforms.Pad(1, padding_mode=\"edge\"),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((MEAN,), (STD,)),\n",
    "        transforms.GaussianBlur(kernel_size=(3, 3)),\n",
    "    ]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    [  # Workaround: stride=1, padding_mode='replicate' is replaced by\n",
    "        # transforms.Pad(1, padding_mode=\"edge\")\n",
    "        transforms.Pad(1, padding_mode=\"edge\"),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((MEAN,), (STD,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(download=True, root=\"./data\", train=True, transform=train_transform)\n",
    "\n",
    "test_dataset = datasets.MNIST(download=True, root=\"./data\", train=False, transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    generator=g,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    generator=g,\n",
    ")\n",
    "\n",
    "plot_dataset(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FHE-Compatible\n",
    "\n",
    "In the next sections of this notebook, we apply the same experimental protocol to both NN-20 and NN-50 models.\n",
    "\n",
    "To make a custom neural network FHE-compatible, it's necessary to quantize both the network and its inputs.\n",
    "\n",
    "Quantization is essentially the process of converting continuous data into integers. This is critical in the context of T-FHE. For custom neural Nntworks (NN), 2 widely methods are supported in concrete ML:\n",
    "\n",
    "- Post-Training Quantization (PTQ): This method involves quantizing a pre-trained floating-point model directly, noted here `fp32_mnist`. While PTQ is straightforward and does not require re-training, it can sometimes compromise the model's performance due to the abrupt adjustment to lower precision. In Concrete Ml, the compilation of the custom NNs is performed through `compile_torch_model` function\n",
    "- Quantization-Aware Training (QAT): Unlike PTQ, QAT integrates quantization into the training cycle, allowing the model to adapt to the quantization effects iteratively. This method often achieves better results but requires a specific framework, designed for neural network quantization. At Concrete ML, Brevitas framework is supported, and the compilation is performed through `compile_brevitas_qat_model` function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the compilation step, the compiler requires an exhaustive set of data, here noted `data_calibration` to evaluate the maximum integer bit-width within the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 30, 30])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_calibration = next(iter(train_loader))[0]\n",
    "\n",
    "data_calibration.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating Point Precision: 20-Layer MNIST\n",
    "\n",
    "Let's strart with the NN-20.\n",
    "\n",
    "The model weights are saved in the file `checkpoints/MNIST/NLP_{nb_layers}/fp32/MNIST_fp32_state_dict.pt`.\n",
    "\n",
    "To train the network, set the `train_model` flag to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating point network: with 20-NN MNIST:\n",
      "99.331% for training set and 98.137% for test set\n"
     ]
    }
   ],
   "source": [
    "nb_layers, train_model = 20, False\n",
    "\n",
    "param = {\n",
    "    \"training\": \"fp32\",\n",
    "    \"dir\": f\"./checkpoints/MNIST/NLP_{nb_layers}/\",\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 0.01,\n",
    "    \"milestones\": [15, 24],\n",
    "    \"gamma\": 0.1,\n",
    "}\n",
    "\n",
    "fp32_mnist = Fp32MNIST(nb_layers=nb_layers).to(DEVICE)\n",
    "\n",
    "if train_model:\n",
    "    assert str(fp32_mnist.nb_layers) in param[\"dir\"]\n",
    "    fp32_mnist = train(fp32_mnist, train_loader, test_loader, param, device=DEVICE)\n",
    "\n",
    "else:\n",
    "    checkpoint = torch.load(f\"{param['dir']}/fp32/MNIST_fp32_state_dict.pt\", map_location=DEVICE)\n",
    "    fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "acc_train = torch_inference(fp32_mnist, train_loader, device=DEVICE)\n",
    "acc_test = torch_inference(fp32_mnist, test_loader, device=DEVICE)\n",
    "\n",
    "print(\n",
    "    f\"Floating point network: with {fp32_mnist.nb_layers}-NN MNIST:\\n\"\n",
    "    f\"{acc_train:.3%} for training set and {acc_test:.3%} for test set\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization: 20-Layer MNIST\n",
    "\n",
    "`QuantMNIST` is the quantized counterpart of the `fp32MNIST` model using [Brevitas]().\n",
    "\n",
    "Quantization aware training can be slow, so a useful strategy is to initialize the quantized network with pre-trained fp32 weights and then fine-tune it. \n",
    "\n",
    "Typically, a few epochs are sufficient to achieve good results with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 4-bits and 20-layers:\n",
      "Accuracy before fine-tuning: 9.750% for the training set and 9.736% for the test set\n",
      "Accuracy after fine-tuning: 97.322% for the training set and 97.165% for the test set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantization with 4-bits\n",
    "bits, nb_layers, finetune_model = 4, 20, False\n",
    "\n",
    "param = {\n",
    "    \"training\": f\"quant_mnist_{bits=}\",\n",
    "    \"dir\": f\"./checkpoints/MNIST/NLP_{nb_layers}/\",\n",
    "    \"epochs\": 5,  # A few epochs are enough to achieve good results\n",
    "    \"lr\": 0.1,\n",
    "    \"milestones\": [1, 3],\n",
    "    \"gamma\": 0.1,\n",
    "}\n",
    "\n",
    "quant_mnist = QuantMNIST(n_bits=bits, nb_layers=nb_layers).to(DEVICE)\n",
    "\n",
    "# Affect the pre-trained  FP32 weights to the equivalent quantized network\n",
    "quant_mnist = mapping_keys(checkpoint, quant_mnist, device=DEVICE)\n",
    "\n",
    "# Assess the quantized model before fine-tuning\n",
    "acc_train_before_ft = torch_inference(quant_mnist, train_loader, device=DEVICE)\n",
    "acc_test_before_ft = torch_inference(quant_mnist, test_loader, device=DEVICE)\n",
    "print(\n",
    "    f\"With {quant_mnist.n_bits}-bits and {quant_mnist.nb_layers}-layers:\\n\"\n",
    "    f\"Accuracy before fine-tuning: {acc_train_before_ft:.3%} \"\n",
    "    f\"for the training set and {acc_test_before_ft:.3%} for the test set\"\n",
    ")\n",
    "\n",
    "# Fine-tune the quantized model\n",
    "if finetune_model:\n",
    "    assert str(quant_mnist.nb_layers) in param[\"dir\"]\n",
    "    quant_mnist = train(quant_mnist, train_loader, test_loader, param, device=DEVICE)\n",
    "\n",
    "# Load the pre-trained quantized model with 20-layers\n",
    "else:\n",
    "    path = f\"{param['dir']}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    "    quant_mnist.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "\n",
    "# Assess the quantized model after fine-tuning\n",
    "acc_train_after_ft = torch_inference(quant_mnist, train_loader, device=DEVICE)\n",
    "acc_test_after_ft = torch_inference(quant_mnist, test_loader, device=DEVICE)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy after fine-tuning: {acc_train_after_ft:.3%} for the training \"\n",
    "    f\"set and {acc_test_after_ft:.3%} for the test set\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after fine-tuning: 97.986% for the training set and 97.726% for the test set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantization with 5-bits\n",
    "bits, nb_layers = 5, 20\n",
    "\n",
    "# Load the pre-trained quantized model with 20-layers\n",
    "path = f\"{param['dir']}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    "\n",
    "quant_mnist = QuantMNIST(n_bits=bits, nb_layers=nb_layers).to(\"cpu\")\n",
    "quant_mnist.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "\n",
    "# Assess the quantized model after fine-tuning\n",
    "acc_train_after_ft = torch_inference(quant_mnist, train_loader, device=DEVICE)\n",
    "acc_test_after_ft = torch_inference(quant_mnist, test_loader, device=DEVICE)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy after fine-tuning: {acc_train_after_ft:.3%} for \"\n",
    "    f\"the training set and {acc_test_after_ft:.3%} for the test set\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST NN-20: Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qat_benchmark(nb_layers=20, data_loader=test_loader, data_calibration=data_calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptq_benchmark(nb_layers=20, data_loader=test_loader, data_calibration=data_calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration with PTQ:\n",
      "{'compile_type': 'PTQ', 'number_of_layers': 20, 'QAT/PTQ_n_bits': 5, 'threshold_nbits': 6.0, 'threshold_method': 'APPROXIMATE', 'max_bits': 15, 'mean_FP32_accuracy': 0.978165064102564, 'mean_disable_accuracy': 0.9729567307692308, 'mean_simulate_accuracy': 0.9724559294871796, 'FHE_timing': 1.8427709062894184, 'input_compression': 0, 'machine': 'PC'}\n",
      "For machie: PC - compile_ method : PTQ\n",
      "Compared to the data from the paper, which recorded for NN-20 and inference time of 115.52 seconds.\n",
      "We observe a significant gain in this new Concrete ML version. The inference time has been reduced to 1.84 seconds.\n",
      "This represents a reduction factor of 63.\n",
      "\n",
      "Best configuration with QAT:\n",
      "{'compile_type': 'QAT', 'number_of_layers': 20, 'QAT/PTQ_n_bits': 5, 'threshold_nbits': 6.0, 'threshold_method': 'APPROXIMATE', 'max_bits': 16, 'mean_FP32_accuracy': 0.9729567307692308, 'mean_disable_accuracy': 0.971153846153846, 'mean_simulate_accuracy': 0.971153846153846, 'FHE_timing': 1.7847444772720338, 'input_compression': 0, 'machine': 'PC'}\n",
      "For machie: PC - compile_ method : QAT\n",
      "Compared to the data from the paper, which recorded for NN-20 and inference time of 115.52 seconds.\n",
      "We observe a significant gain in this new Concrete ML version. The inference time has been reduced to 1.78 seconds.\n",
      "This represents a reduction factor of 65.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compile_type</th>\n",
       "      <th>number_of_layers</th>\n",
       "      <th>QAT/PTQ_n_bits</th>\n",
       "      <th>threshold_nbits</th>\n",
       "      <th>threshold_method</th>\n",
       "      <th>max_bits</th>\n",
       "      <th>mean_FP32_accuracy</th>\n",
       "      <th>mean_disable_accuracy</th>\n",
       "      <th>mean_simulate_accuracy</th>\n",
       "      <th>FHE_timing</th>\n",
       "      <th>input_compression</th>\n",
       "      <th>machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>15</td>\n",
       "      <td>0.978165</td>\n",
       "      <td>0.972957</td>\n",
       "      <td>0.972456</td>\n",
       "      <td>1.842771</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>17</td>\n",
       "      <td>0.976362</td>\n",
       "      <td>0.975361</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>2.169491</td>\n",
       "      <td>1</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>17</td>\n",
       "      <td>0.976362</td>\n",
       "      <td>0.975761</td>\n",
       "      <td>0.948618</td>\n",
       "      <td>2.184025</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>QAT</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>16</td>\n",
       "      <td>0.972957</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>1.784744</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>QAT</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>16</td>\n",
       "      <td>0.972155</td>\n",
       "      <td>0.969351</td>\n",
       "      <td>0.930689</td>\n",
       "      <td>2.295221</td>\n",
       "      <td>1</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>QAT</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>16</td>\n",
       "      <td>0.971254</td>\n",
       "      <td>0.970453</td>\n",
       "      <td>0.970453</td>\n",
       "      <td>3.121205</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   compile_type  number_of_layers  QAT/PTQ_n_bits  threshold_nbits  \\\n",
       "40          PTQ                20               5              6.0   \n",
       "70          PTQ                20               6              6.0   \n",
       "46          PTQ                20               6              6.0   \n",
       "22          QAT                20               5              6.0   \n",
       "58          QAT                20               5              6.0   \n",
       "23          QAT                20               5              6.0   \n",
       "\n",
       "   threshold_method  max_bits  mean_FP32_accuracy  mean_disable_accuracy  \\\n",
       "40      APPROXIMATE        15            0.978165               0.972957   \n",
       "70      APPROXIMATE        17            0.976362               0.975361   \n",
       "46      APPROXIMATE        17            0.976362               0.975761   \n",
       "22      APPROXIMATE        16            0.972957               0.971154   \n",
       "58      APPROXIMATE        16            0.972155               0.969351   \n",
       "23            EXACT        16            0.971254               0.970453   \n",
       "\n",
       "    mean_simulate_accuracy  FHE_timing  input_compression machine  \n",
       "40                0.972456    1.842771                  0      PC  \n",
       "70                0.945813    2.169491                  1      PC  \n",
       "46                0.948618    2.184025                  0      PC  \n",
       "22                0.971154    1.784744                  0      PC  \n",
       "58                0.930689    2.295221                  1      PC  \n",
       "23                0.970453    3.121205                  0      PC  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_configuration(machine_type=\"PC\", paper_notes=PAPER_NOTES_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration with PTQ:\n",
      "{'compile_type': 'PTQ', 'number_of_layers': 20, 'QAT/PTQ_n_bits': 6, 'threshold_nbits': 6.0, 'threshold_method': 'APPROXIMATE', 'max_bits': 17, 'mean_FP32_accuracy': 0.9763621794871796, 'mean_disable_accuracy': 0.9752604166666666, 'mean_simulate_accuracy': 0.963241185897436, 'FHE_timing': 0.2431224187215169, 'input_compression': 0, 'machine': 'HP7C'}\n",
      "For machie: HP7C - compile_ method : PTQ\n",
      "Compared to the data from the paper, which recorded for NN-20 and inference time of 115.52 seconds.\n",
      "We observe a significant gain in this new Concrete ML version. The inference time has been reduced to 0.24 seconds.\n",
      "This represents a reduction factor of 475.\n",
      "\n",
      "Best configuration with QAT:\n",
      "{'compile_type': 'QAT', 'number_of_layers': 20, 'QAT/PTQ_n_bits': 5, 'threshold_nbits': 6.0, 'threshold_method': 'APPROXIMATE', 'max_bits': 16, 'mean_FP32_accuracy': 0.9736578525641024, 'mean_disable_accuracy': 0.9697516025641024, 'mean_simulate_accuracy': 0.9700520833333334, 'FHE_timing': 0.3180721044540405, 'input_compression': 0, 'machine': 'HP7C'}\n",
      "For machie: HP7C - compile_ method : QAT\n",
      "Compared to the data from the paper, which recorded for NN-20 and inference time of 115.52 seconds.\n",
      "We observe a significant gain in this new Concrete ML version. The inference time has been reduced to 0.32 seconds.\n",
      "This represents a reduction factor of 363.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compile_type</th>\n",
       "      <th>number_of_layers</th>\n",
       "      <th>QAT/PTQ_n_bits</th>\n",
       "      <th>threshold_nbits</th>\n",
       "      <th>threshold_method</th>\n",
       "      <th>max_bits</th>\n",
       "      <th>mean_FP32_accuracy</th>\n",
       "      <th>mean_disable_accuracy</th>\n",
       "      <th>mean_simulate_accuracy</th>\n",
       "      <th>FHE_timing</th>\n",
       "      <th>input_compression</th>\n",
       "      <th>machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>17</td>\n",
       "      <td>0.976362</td>\n",
       "      <td>0.975260</td>\n",
       "      <td>0.963241</td>\n",
       "      <td>0.243122</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>15</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.975160</td>\n",
       "      <td>0.972256</td>\n",
       "      <td>0.281484</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>15</td>\n",
       "      <td>0.975761</td>\n",
       "      <td>0.973357</td>\n",
       "      <td>0.970954</td>\n",
       "      <td>0.376488</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>QAT</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>16</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.969752</td>\n",
       "      <td>0.970052</td>\n",
       "      <td>0.318072</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>QAT</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>16</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.969752</td>\n",
       "      <td>0.970052</td>\n",
       "      <td>0.319026</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>QAT</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>16</td>\n",
       "      <td>0.974659</td>\n",
       "      <td>0.970753</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>0.341152</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    compile_type  number_of_layers  QAT/PTQ_n_bits  threshold_nbits  \\\n",
       "128          PTQ                20               6              6.0   \n",
       "94           PTQ                20               5              6.0   \n",
       "95           PTQ                20               5              6.0   \n",
       "125          QAT                20               5              6.0   \n",
       "121          QAT                20               5              6.0   \n",
       "88           QAT                20               5              6.0   \n",
       "\n",
       "    threshold_method  max_bits  mean_FP32_accuracy  mean_disable_accuracy  \\\n",
       "128      APPROXIMATE        17            0.976362               0.975260   \n",
       "94       APPROXIMATE        15            0.976562               0.975160   \n",
       "95             EXACT        15            0.975761               0.973357   \n",
       "125      APPROXIMATE        16            0.973658               0.969752   \n",
       "121      APPROXIMATE        16            0.973658               0.969752   \n",
       "88       APPROXIMATE        16            0.974659               0.970753   \n",
       "\n",
       "     mean_simulate_accuracy  FHE_timing  input_compression machine  \n",
       "128                0.963241    0.243122                  0    HP7C  \n",
       "94                 0.972256    0.281484                  0    HP7C  \n",
       "95                 0.970954    0.376488                  0    HP7C  \n",
       "125                0.970052    0.318072                  0    HP7C  \n",
       "121                0.970052    0.319026                  0    HP7C  \n",
       "88                 0.971154    0.341152                  0    HP7C  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_configuration(machine_type=\"HP7C\", paper_notes=PAPER_NOTES_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating Point Precision: 50-Layer MNIST\n",
    "\n",
    "Same protocol as the MNIST 20-layers.\n",
    "\n",
    "The network is trained in floationg point precision, then quantized with the QAT method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layers = 50\n",
    "param = {\"dir\": f\"./checkpoints/MNIST/NLP_{nb_layers}/\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 - with 50-NN: 99.331% for training set and 98.137% for test set\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(f\"{param['dir']}/fp32/MNIST_fp32_state_dict.pt\", map_location=DEVICE)\n",
    "\n",
    "fp32_mnist = Fp32MNIST(nb_layers=nb_layers).to(DEVICE)\n",
    "fp32_mnist.load_state_dict(checkpoint)\n",
    "\n",
    "acc_after_ft_train = torch_inference(fp32_mnist, train_loader, device=DEVICE)\n",
    "acc_after_ft_test = torch_inference(fp32_mnist, test_loader, device=DEVICE)\n",
    "\n",
    "print(\n",
    "    f\"FP32 - with {fp32_mnist.nb_layers}-NN: {acc_train:.3%} for \"\n",
    "    f\"training set and {acc_test:.3%} for test set\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization: 50-Layer MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after fine-tuning: 93.400% for the training set and 93.409% for the test set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# quantization with 4-bits\n",
    "bits, nb_layers = 4, 50\n",
    "\n",
    "# Load pre-trained quantized model\n",
    "path = f\"{param['dir']}/quant_mnist_{bits=}/MNIST_quant_mnist_{bits=}_state_dict.pt\"\n",
    "\n",
    "quant_mnist = QuantMNIST(n_bits=bits, nb_layers=nb_layers).to(DEVICE)\n",
    "quant_mnist.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "\n",
    "# Assess the model after fine-tuning\n",
    "acc_train_after_ft = torch_inference(quant_mnist, train_loader, device=DEVICE)\n",
    "acc_test_after_ft = torch_inference(quant_mnist, test_loader, device=DEVICE)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy after fine-tuning: {acc_train_after_ft:.3%} for \"\n",
    "    f\"the training set and {acc_test_after_ft:.3%} for the test set\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST NN-50: Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptq_benchmark(nb_layers=50, data_loader=test_loader, data_calibration=data_calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qat_benchmark(nb_layers=50, data_loader=test_loader, data_calibration=data_calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration with PTQ:\n",
      "{'compile_type': 'PTQ', 'number_of_layers': 50, 'QAT/PTQ_n_bits': 5, 'threshold_n_bits': 7.0, 'threshold_method': 'APPROXIMATE', 'max_bits': 15, 'mean_FP32_accuracy': 0.9665464743589745, 'mean_disable_accuracy': 0.9579326923076924, 'mean_simulate_accuracy': 0.9549278846153846, 'FHE_timing': 11.522332433859509, 'input_compression': 0, 'machine': 'PC'}\n",
      "For machie: PC - compile_ method : PTQ\n",
      "Compared to the data from the paper, which recorded for NN-50 and inference time of 233.55 seconds.\n",
      "We observe a significant gain in this new Concrete ML version. The inference time has been reduced to 11.52 seconds.\n",
      "This represents a reduction factor of 20.\n",
      "\n",
      "Best configuration with QAT:\n",
      "{'compile_type': 'QAT', 'number_of_layers': 50, 'QAT/PTQ_n_bits': 4, 'threshold_n_bits': 5.0, 'threshold_method': 'APPROXIMATE', 'max_bits': 14, 'mean_FP32_accuracy': 0.88671875, 'mean_disable_accuracy': 0.8990384615384616, 'mean_simulate_accuracy': 0.8973357371794872, 'FHE_timing': 1.6271005471547444, 'input_compression': 1, 'machine': 'PC'}\n",
      "For machie: PC - compile_ method : QAT\n",
      "Compared to the data from the paper, which recorded for NN-50 and inference time of 233.55 seconds.\n",
      "We observe a significant gain in this new Concrete ML version. The inference time has been reduced to 1.63 seconds.\n",
      "This represents a reduction factor of 144.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compile_type</th>\n",
       "      <th>number_of_layers</th>\n",
       "      <th>QAT/PTQ_n_bits</th>\n",
       "      <th>threshold_n_bits</th>\n",
       "      <th>threshold_method</th>\n",
       "      <th>max_bits</th>\n",
       "      <th>mean_FP32_accuracy</th>\n",
       "      <th>mean_disable_accuracy</th>\n",
       "      <th>mean_simulate_accuracy</th>\n",
       "      <th>FHE_timing</th>\n",
       "      <th>input_compression</th>\n",
       "      <th>machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>15</td>\n",
       "      <td>0.966546</td>\n",
       "      <td>0.957933</td>\n",
       "      <td>0.954928</td>\n",
       "      <td>11.522332</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>15</td>\n",
       "      <td>0.967147</td>\n",
       "      <td>0.965946</td>\n",
       "      <td>0.933093</td>\n",
       "      <td>14.316687</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>14</td>\n",
       "      <td>0.886719</td>\n",
       "      <td>0.899038</td>\n",
       "      <td>0.897336</td>\n",
       "      <td>1.627101</td>\n",
       "      <td>1</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>14</td>\n",
       "      <td>0.889924</td>\n",
       "      <td>0.898838</td>\n",
       "      <td>0.897135</td>\n",
       "      <td>1.701109</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>14</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.898838</td>\n",
       "      <td>0.897135</td>\n",
       "      <td>1.925014</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>14</td>\n",
       "      <td>0.886719</td>\n",
       "      <td>0.899038</td>\n",
       "      <td>0.898938</td>\n",
       "      <td>3.022286</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>14</td>\n",
       "      <td>0.889323</td>\n",
       "      <td>0.888522</td>\n",
       "      <td>0.889523</td>\n",
       "      <td>3.779341</td>\n",
       "      <td>1</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>14</td>\n",
       "      <td>0.890425</td>\n",
       "      <td>0.935397</td>\n",
       "      <td>0.935397</td>\n",
       "      <td>4.680738</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>14</td>\n",
       "      <td>0.889323</td>\n",
       "      <td>0.888522</td>\n",
       "      <td>0.888522</td>\n",
       "      <td>4.700674</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>14</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.920072</td>\n",
       "      <td>0.920072</td>\n",
       "      <td>4.812003</td>\n",
       "      <td>1</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   compile_type  number_of_layers  QAT/PTQ_n_bits  threshold_n_bits  \\\n",
       "13          PTQ                50               5               7.0   \n",
       "14          PTQ                50               5               7.0   \n",
       "40          QAT                50               4               5.0   \n",
       "38          QAT                50               4               5.0   \n",
       "37          QAT                50               4               5.0   \n",
       "15          QAT                50               4               5.0   \n",
       "42          QAT                50               4               6.0   \n",
       "23          QAT                50               4               6.0   \n",
       "17          QAT                50               4               6.0   \n",
       "43          QAT                50               4               6.0   \n",
       "\n",
       "   threshold_method  max_bits  mean_FP32_accuracy  mean_disable_accuracy  \\\n",
       "13      APPROXIMATE        15            0.966546               0.957933   \n",
       "14            EXACT        15            0.967147               0.965946   \n",
       "40      APPROXIMATE        14            0.886719               0.899038   \n",
       "38      APPROXIMATE        14            0.889924               0.898838   \n",
       "37      APPROXIMATE        14            0.882913               0.898838   \n",
       "15            EXACT        14            0.886719               0.899038   \n",
       "42      APPROXIMATE        14            0.889323               0.888522   \n",
       "23            EXACT        14            0.890425               0.935397   \n",
       "17            EXACT        14            0.889323               0.888522   \n",
       "43            EXACT        14            0.882913               0.920072   \n",
       "\n",
       "    mean_simulate_accuracy  FHE_timing  input_compression machine  \n",
       "13                0.954928   11.522332                  0      PC  \n",
       "14                0.933093   14.316687                  0      PC  \n",
       "40                0.897336    1.627101                  1      PC  \n",
       "38                0.897135    1.701109                  0      PC  \n",
       "37                0.897135    1.925014                  0      PC  \n",
       "15                0.898938    3.022286                  0      PC  \n",
       "42                0.889523    3.779341                  1      PC  \n",
       "23                0.935397    4.680738                  0      PC  \n",
       "17                0.888522    4.700674                  0      PC  \n",
       "43                0.920072    4.812003                  1      PC  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_configuration(machine_type=\"PC\", paper_notes=PAPER_NOTES_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration with PTQ:\n",
      "{'compile_type': 'PTQ', 'number_of_layers': 50, 'QAT/PTQ_n_bits': 5, 'threshold_n_bits': 6.0, 'threshold_method': 'APPROXIMATE', 'max_bits': 15, 'mean_FP32_accuracy': 0.9678485576923076, 'mean_disable_accuracy': 0.9588341346153846, 'mean_simulate_accuracy': 0.951622596153846, 'FHE_timing': 0.8221531271934509, 'input_compression': 0, 'machine': 'HP7C'}\n",
      "For machie: HP7C - compile_ method : PTQ\n",
      "Compared to the data from the paper, which recorded for NN-50 and inference time of 233.55 seconds.\n",
      "We observe a significant gain in this new Concrete ML version. The inference time has been reduced to 0.82 seconds.\n",
      "This represents a reduction factor of 284.\n",
      "\n",
      "Best configuration with QAT:\n",
      "{'compile_type': 'QAT', 'number_of_layers': 50, 'QAT/PTQ_n_bits': 4, 'threshold_n_bits': 5.0, 'threshold_method': 'APPROXIMATE', 'max_bits': 14, 'mean_FP32_accuracy': 0.8862179487179487, 'mean_disable_accuracy': 0.879707532051282, 'mean_simulate_accuracy': 0.8645833333333334, 'FHE_timing': 0.2637086192766825, 'input_compression': 0, 'machine': 'HP7C'}\n",
      "For machie: HP7C - compile_ method : QAT\n",
      "Compared to the data from the paper, which recorded for NN-50 and inference time of 233.55 seconds.\n",
      "We observe a significant gain in this new Concrete ML version. The inference time has been reduced to 0.26 seconds.\n",
      "This represents a reduction factor of 886.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compile_type</th>\n",
       "      <th>number_of_layers</th>\n",
       "      <th>QAT/PTQ_n_bits</th>\n",
       "      <th>threshold_n_bits</th>\n",
       "      <th>threshold_method</th>\n",
       "      <th>max_bits</th>\n",
       "      <th>mean_FP32_accuracy</th>\n",
       "      <th>mean_disable_accuracy</th>\n",
       "      <th>mean_simulate_accuracy</th>\n",
       "      <th>FHE_timing</th>\n",
       "      <th>input_compression</th>\n",
       "      <th>machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>15</td>\n",
       "      <td>0.967849</td>\n",
       "      <td>0.958834</td>\n",
       "      <td>0.951623</td>\n",
       "      <td>0.822153</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>17</td>\n",
       "      <td>0.967849</td>\n",
       "      <td>0.958233</td>\n",
       "      <td>0.962540</td>\n",
       "      <td>1.149665</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>17</td>\n",
       "      <td>0.964243</td>\n",
       "      <td>0.960136</td>\n",
       "      <td>0.947616</td>\n",
       "      <td>1.326429</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>19</td>\n",
       "      <td>0.965144</td>\n",
       "      <td>0.958433</td>\n",
       "      <td>0.956731</td>\n",
       "      <td>2.332958</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>19</td>\n",
       "      <td>0.969852</td>\n",
       "      <td>0.963842</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>2.352652</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>17</td>\n",
       "      <td>0.967849</td>\n",
       "      <td>0.962039</td>\n",
       "      <td>0.942508</td>\n",
       "      <td>2.381191</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>17</td>\n",
       "      <td>0.966947</td>\n",
       "      <td>0.964443</td>\n",
       "      <td>0.953626</td>\n",
       "      <td>2.689226</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>PTQ</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>19</td>\n",
       "      <td>0.966947</td>\n",
       "      <td>0.958934</td>\n",
       "      <td>0.957432</td>\n",
       "      <td>8.223564</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>14</td>\n",
       "      <td>0.886218</td>\n",
       "      <td>0.879708</td>\n",
       "      <td>0.864583</td>\n",
       "      <td>0.263709</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>15</td>\n",
       "      <td>0.889623</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.882312</td>\n",
       "      <td>0.302718</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>15</td>\n",
       "      <td>0.889623</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.882312</td>\n",
       "      <td>0.303311</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>15</td>\n",
       "      <td>0.889623</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.882312</td>\n",
       "      <td>0.304650</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>APPROXIMATE</td>\n",
       "      <td>15</td>\n",
       "      <td>0.889623</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.882312</td>\n",
       "      <td>0.304650</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>15</td>\n",
       "      <td>0.889623</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.503104</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>15</td>\n",
       "      <td>0.889623</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.504589</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>QAT</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EXACT</td>\n",
       "      <td>15</td>\n",
       "      <td>0.889623</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.882913</td>\n",
       "      <td>0.505852</td>\n",
       "      <td>0</td>\n",
       "      <td>HP7C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    compile_type  number_of_layers  QAT/PTQ_n_bits  threshold_n_bits  \\\n",
       "58           PTQ                50               5               6.0   \n",
       "79           PTQ                50               6               6.0   \n",
       "80           PTQ                50               6               6.0   \n",
       "84           PTQ                50               7               6.0   \n",
       "179          PTQ                50               7               6.0   \n",
       "81           PTQ                50               6               7.0   \n",
       "82           PTQ                50               6               7.0   \n",
       "176          PTQ                50               7               6.0   \n",
       "146          QAT                50               4               5.0   \n",
       "125          QAT                50               4               5.0   \n",
       "111          QAT                50               4               5.0   \n",
       "160          QAT                50               4               5.0   \n",
       "162          QAT                50               4               5.0   \n",
       "87           QAT                50               4               5.0   \n",
       "63           QAT                50               4               5.0   \n",
       "66           QAT                50               4               5.0   \n",
       "\n",
       "    threshold_method  max_bits  mean_FP32_accuracy  mean_disable_accuracy  \\\n",
       "58       APPROXIMATE        15            0.967849               0.958834   \n",
       "79       APPROXIMATE        17            0.967849               0.958233   \n",
       "80             EXACT        17            0.964243               0.960136   \n",
       "84       APPROXIMATE        19            0.965144               0.958433   \n",
       "179      APPROXIMATE        19            0.969852               0.963842   \n",
       "81       APPROXIMATE        17            0.967849               0.962039   \n",
       "82             EXACT        17            0.966947               0.964443   \n",
       "176      APPROXIMATE        19            0.966947               0.958934   \n",
       "146      APPROXIMATE        14            0.886218               0.879708   \n",
       "125      APPROXIMATE        15            0.889623               0.882913   \n",
       "111      APPROXIMATE        15            0.889623               0.882913   \n",
       "160      APPROXIMATE        15            0.889623               0.882913   \n",
       "162      APPROXIMATE        15            0.889623               0.882913   \n",
       "87             EXACT        15            0.889623               0.882913   \n",
       "63             EXACT        15            0.889623               0.882913   \n",
       "66             EXACT        15            0.889623               0.882913   \n",
       "\n",
       "     mean_simulate_accuracy  FHE_timing  input_compression machine  \n",
       "58                 0.951623    0.822153                  0    HP7C  \n",
       "79                 0.962540    1.149665                  0    HP7C  \n",
       "80                 0.947616    1.326429                  0    HP7C  \n",
       "84                 0.956731    2.332958                  0    HP7C  \n",
       "179                0.942708    2.352652                  0    HP7C  \n",
       "81                 0.942508    2.381191                  0    HP7C  \n",
       "82                 0.953626    2.689226                  0    HP7C  \n",
       "176                0.957432    8.223564                  0    HP7C  \n",
       "146                0.864583    0.263709                  0    HP7C  \n",
       "125                0.882312    0.302718                  0    HP7C  \n",
       "111                0.882312    0.303311                  0    HP7C  \n",
       "160                0.882312    0.304650                  0    HP7C  \n",
       "162                0.882312    0.304650                  0    HP7C  \n",
       "87                 0.882913    0.503104                  0    HP7C  \n",
       "63                 0.882913    0.504589                  0    HP7C  \n",
       "66                 0.882913    0.505852                  0    HP7C  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_configuration(machine_type=\"HP7C\", paper_notes=PAPER_NOTES_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
